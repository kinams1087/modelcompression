{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/kuangliu/pytorch-cifar.git\n",
        "%cd pytorch-cifar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7HRVmipKGQt",
        "outputId": "2cb103b6-e9c8-4c51-a827-30dd87bedfb5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pytorch-cifar'...\n",
            "remote: Enumerating objects: 382, done.\u001b[K\n",
            "remote: Total 382 (delta 0), reused 0 (delta 0), pack-reused 382 (from 1)\u001b[K\n",
            "Receiving objects: 100% (382/382), 94.67 KiB | 9.47 MiB/s, done.\n",
            "Resolving deltas: 100% (182/182), done.\n",
            "/content/pytorch-cifar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "\n",
        "from models import *"
      ],
      "metadata": {
        "id": "m13zcWsOKo1G"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    from models.resnet import ResNet18\n",
        "except Exception as e:\n",
        "    # 로컬에 models가 없으면 간단 안내\n",
        "    raise RuntimeError(\"models/ 디렉터리가 필요합니다. 깃 레포를 클론하거나 models를 복사하세요.\") from e"
      ],
      "metadata": {
        "id": "RP97x3E1LvsF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "best_acc = 0  # best test accuracy\n",
        "start_epoch = 0  # start from epoch 0 or last checkpoint epoch"
      ],
      "metadata": {
        "id": "K8n4iYMdM5EK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data\n",
        "print('==> Preparing data..')\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(\n",
        "    trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(\n",
        "    testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
        "           'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6xGafZbM-oD",
        "outputId": "0f8e5b77-7d07-49b9-c365-dadec21feb04"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Preparing data..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:05<00:00, 29.9MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model\n",
        "print('==> Building model..')\n",
        "net = ResNet18()\n",
        "net = net.to(device)\n",
        "if device == 'cuda':\n",
        "    net = torch.nn.DataParallel(net)\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.1,\n",
        "                      momentum=0.9, weight_decay=5e-4)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFm3OURiNC4C",
        "outputId": "4714590f-433f-440a-cda0-3b9382f683c2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Building model..\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, time\n",
        "\n",
        "# 진행바용 전역 변수 초기화\n",
        "TOTAL_BAR_LENGTH = 65.\n",
        "last_time = time.time()\n",
        "begin_time = last_time\n",
        "\n",
        "def format_time(seconds):\n",
        "    \"\"\"초 단위 시간을 h m s 포맷으로 바꾸는 함수\"\"\"\n",
        "    L = []\n",
        "    for count, unit in [(3600,'h'), (60,'m'), (1,'s')]:\n",
        "        if seconds >= count:\n",
        "            val = int(seconds // count)\n",
        "            seconds -= val * count\n",
        "            L.append(f\"{val}{unit}\")\n",
        "    return ' '.join(L) if L else '0s'\n",
        "\n",
        "def progress_bar(current, total, msg=None):\n",
        "    \"\"\"\n",
        "    progress_bar(i, len(loader), '메시지') 형태로 호출\n",
        "    예시:\n",
        "        for i, (x, y) in enumerate(train_loader):\n",
        "            progress_bar(i, len(train_loader), f\"loss {loss.item():.3f}\")\n",
        "    \"\"\"\n",
        "    global last_time, begin_time\n",
        "    if current == 0:\n",
        "        begin_time = time.time()\n",
        "\n",
        "    cur_len = int(TOTAL_BAR_LENGTH * current / total)\n",
        "    rest_len = int(TOTAL_BAR_LENGTH - cur_len) - 1\n",
        "    bar = '[' + '=' * cur_len + '>' + '.' * rest_len + ']'\n",
        "\n",
        "    cur_time = time.time()\n",
        "    step_time = cur_time - last_time\n",
        "    last_time = cur_time\n",
        "    tot_time = cur_time - begin_time\n",
        "\n",
        "    text = f\"\\r{bar} Step: {format_time(step_time)} | Tot: {format_time(tot_time)}\"\n",
        "    if msg:\n",
        "        text += f\" | {msg}\"\n",
        "\n",
        "    # Colab은 stdout flush를 꼭 해줘야 바로 출력됨\n",
        "    sys.stdout.write(text)\n",
        "    sys.stdout.flush()\n",
        "    if current == total - 1:\n",
        "        sys.stdout.write('\\n')"
      ],
      "metadata": {
        "id": "Pxf_3vhClcgT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "def train(epoch):\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "                     % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "\n",
        "\n",
        "def test(epoch):\n",
        "    global best_acc\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "            progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "                         % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "\n",
        "    # Save checkpoint.\n",
        "    acc = 100.*correct/total\n",
        "    if acc > best_acc:\n",
        "        print('Saving..')\n",
        "        state = {\n",
        "            'net': net.state_dict(),\n",
        "            'acc': acc,\n",
        "            'epoch': epoch,\n",
        "        }\n",
        "        if not os.path.isdir('checkpoint'):\n",
        "            os.mkdir('checkpoint')\n",
        "        torch.save(state, './checkpoint/ResNet18.pth')\n",
        "        best_acc = acc"
      ],
      "metadata": {
        "id": "7U-IvlXUNXOJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(200):\n",
        "    train(epoch)\n",
        "    test(epoch)\n",
        "    scheduler.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5LeablUOHcM",
        "outputId": "1dd18a06-c066-4107-e0a1-d6845b3709fb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 0\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 1.845 | Acc: 33.542% (16771/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 1.739 | Acc: 39.400% (3940/10000)\n",
            "Saving..\n",
            "\n",
            "Epoch: 1\n",
            "[================================================================>] Step: 0s | Tot: 10s | Loss: 1.368 | Acc: 50.086% (25043/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 1.240 | Acc: 54.490% (5449/10000)\n",
            "Saving..\n",
            "\n",
            "Epoch: 2\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 1.095 | Acc: 61.028% (30514/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 1.271 | Acc: 56.080% (5608/10000)\n",
            "Saving..\n",
            "\n",
            "Epoch: 3\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.918 | Acc: 67.732% (33866/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 1.006 | Acc: 65.210% (6521/10000)\n",
            "Saving..\n",
            "\n",
            "Epoch: 4\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.770 | Acc: 73.166% (36583/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.799 | Acc: 73.380% (7338/10000)\n",
            "Saving..\n",
            "\n",
            "Epoch: 5\n",
            "[================================================================>] Step: 0s | Tot: 10s | Loss: 0.659 | Acc: 77.140% (38570/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.638 | Acc: 77.940% (7794/10000)\n",
            "Saving..\n",
            "\n",
            "Epoch: 6\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.592 | Acc: 79.586% (39793/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.758 | Acc: 74.060% (7406/10000)\n",
            "\n",
            "Epoch: 7\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.552 | Acc: 80.952% (40476/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.580 | Acc: 80.080% (8008/10000)\n",
            "Saving..\n",
            "\n",
            "Epoch: 8\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.519 | Acc: 82.216% (41108/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.649 | Acc: 77.820% (7782/10000)\n",
            "\n",
            "Epoch: 9\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.503 | Acc: 82.760% (41380/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.694 | Acc: 76.840% (7684/10000)\n",
            "\n",
            "Epoch: 10\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.486 | Acc: 83.300% (41650/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.549 | Acc: 81.270% (8127/10000)\n",
            "Saving..\n",
            "\n",
            "Epoch: 11\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.462 | Acc: 84.132% (42066/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.593 | Acc: 80.480% (8048/10000)\n",
            "\n",
            "Epoch: 12\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.451 | Acc: 84.666% (42333/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.667 | Acc: 77.170% (7717/10000)\n",
            "\n",
            "Epoch: 13\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.435 | Acc: 85.180% (42590/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.548 | Acc: 82.290% (8229/10000)\n",
            "Saving..\n",
            "\n",
            "Epoch: 14\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.431 | Acc: 85.166% (42583/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.615 | Acc: 80.210% (8021/10000)\n",
            "\n",
            "Epoch: 15\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.421 | Acc: 85.654% (42827/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.526 | Acc: 82.380% (8238/10000)\n",
            "Saving..\n",
            "\n",
            "Epoch: 16\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.408 | Acc: 85.964% (42982/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.820 | Acc: 75.850% (7585/10000)\n",
            "\n",
            "Epoch: 17\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.406 | Acc: 86.124% (43062/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.582 | Acc: 81.210% (8121/10000)\n",
            "\n",
            "Epoch: 18\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.396 | Acc: 86.428% (43214/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.609 | Acc: 79.250% (7925/10000)\n",
            "\n",
            "Epoch: 19\n",
            "[================================================================>] Step: 0s | Tot: 10s | Loss: 0.388 | Acc: 86.982% (43491/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.527 | Acc: 82.610% (8261/10000)\n",
            "Saving..\n",
            "\n",
            "Epoch: 20\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.387 | Acc: 86.820% (43410/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.555 | Acc: 82.180% (8218/10000)\n",
            "\n",
            "Epoch: 21\n",
            "[================================================================>] Step: 0s | Tot: 10s | Loss: 0.377 | Acc: 87.108% (43554/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.514 | Acc: 82.760% (8276/10000)\n",
            "Saving..\n",
            "\n",
            "Epoch: 22\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.371 | Acc: 87.306% (43653/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.641 | Acc: 79.180% (7918/10000)\n",
            "\n",
            "Epoch: 23\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.372 | Acc: 87.342% (43671/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.583 | Acc: 80.770% (8077/10000)\n",
            "\n",
            "Epoch: 24\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.361 | Acc: 87.626% (43813/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.492 | Acc: 83.630% (8363/10000)\n",
            "Saving..\n",
            "\n",
            "Epoch: 25\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.368 | Acc: 87.344% (43672/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.516 | Acc: 82.500% (8250/10000)\n",
            "\n",
            "Epoch: 26\n",
            "[================================================================>] Step: 0s | Tot: 10s | Loss: 0.361 | Acc: 87.752% (43876/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.530 | Acc: 83.060% (8306/10000)\n",
            "\n",
            "Epoch: 27\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.347 | Acc: 88.182% (44091/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.493 | Acc: 83.700% (8370/10000)\n",
            "Saving..\n",
            "\n",
            "Epoch: 28\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.350 | Acc: 88.040% (44020/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.480 | Acc: 84.350% (8435/10000)\n",
            "Saving..\n",
            "\n",
            "Epoch: 29\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.348 | Acc: 87.988% (43994/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.489 | Acc: 83.690% (8369/10000)\n",
            "\n",
            "Epoch: 30\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.346 | Acc: 88.096% (44048/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.503 | Acc: 83.220% (8322/10000)\n",
            "\n",
            "Epoch: 31\n",
            "[================================================================>] Step: 0s | Tot: 10s | Loss: 0.344 | Acc: 88.258% (44129/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.454 | Acc: 84.690% (8469/10000)\n",
            "Saving..\n",
            "\n",
            "Epoch: 32\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.336 | Acc: 88.704% (44352/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.510 | Acc: 83.060% (8306/10000)\n",
            "\n",
            "Epoch: 33\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.334 | Acc: 88.590% (44295/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.513 | Acc: 82.890% (8289/10000)\n",
            "\n",
            "Epoch: 34\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.331 | Acc: 88.712% (44356/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.490 | Acc: 83.680% (8368/10000)\n",
            "\n",
            "Epoch: 35\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.331 | Acc: 88.836% (44418/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.488 | Acc: 83.330% (8333/10000)\n",
            "\n",
            "Epoch: 36\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.329 | Acc: 88.926% (44463/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.436 | Acc: 85.680% (8568/10000)\n",
            "Saving..\n",
            "\n",
            "Epoch: 37\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.327 | Acc: 88.966% (44483/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.412 | Acc: 85.890% (8589/10000)\n",
            "Saving..\n",
            "\n",
            "Epoch: 38\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.326 | Acc: 88.840% (44420/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.488 | Acc: 83.500% (8350/10000)\n",
            "\n",
            "Epoch: 39\n",
            "[================================================================>] Step: 0s | Tot: 10s | Loss: 0.323 | Acc: 88.944% (44472/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.532 | Acc: 82.560% (8256/10000)\n",
            "\n",
            "Epoch: 40\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.318 | Acc: 89.058% (44529/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.651 | Acc: 80.390% (8039/10000)\n",
            "\n",
            "Epoch: 41\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.316 | Acc: 89.172% (44586/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.526 | Acc: 82.940% (8294/10000)\n",
            "\n",
            "Epoch: 42\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.315 | Acc: 89.256% (44628/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.506 | Acc: 83.440% (8344/10000)\n",
            "\n",
            "Epoch: 43\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.313 | Acc: 89.384% (44692/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.424 | Acc: 85.800% (8580/10000)\n",
            "\n",
            "Epoch: 44\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.312 | Acc: 89.370% (44685/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.510 | Acc: 83.520% (8352/10000)\n",
            "\n",
            "Epoch: 45\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.307 | Acc: 89.438% (44719/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.398 | Acc: 86.620% (8662/10000)\n",
            "Saving..\n",
            "\n",
            "Epoch: 46\n",
            "[================================================================>] Step: 0s | Tot: 10s | Loss: 0.309 | Acc: 89.528% (44764/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.373 | Acc: 87.690% (8769/10000)\n",
            "Saving..\n",
            "\n",
            "Epoch: 47\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.302 | Acc: 89.698% (44849/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.376 | Acc: 87.660% (8766/10000)\n",
            "\n",
            "Epoch: 48\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.297 | Acc: 89.950% (44975/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.416 | Acc: 86.460% (8646/10000)\n",
            "\n",
            "Epoch: 49\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.303 | Acc: 89.588% (44794/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.447 | Acc: 85.420% (8542/10000)\n",
            "\n",
            "Epoch: 50\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.300 | Acc: 89.634% (44817/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.375 | Acc: 87.450% (8745/10000)\n",
            "\n",
            "Epoch: 51\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.300 | Acc: 89.736% (44868/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.525 | Acc: 83.700% (8370/10000)\n",
            "\n",
            "Epoch: 52\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.296 | Acc: 90.022% (45011/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.537 | Acc: 83.730% (8373/10000)\n",
            "\n",
            "Epoch: 53\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.290 | Acc: 90.096% (45048/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.438 | Acc: 85.770% (8577/10000)\n",
            "\n",
            "Epoch: 54\n",
            "[================================================================>] Step: 0s | Tot: 10s | Loss: 0.290 | Acc: 90.124% (45062/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.420 | Acc: 86.800% (8680/10000)\n",
            "\n",
            "Epoch: 55\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.295 | Acc: 89.936% (44968/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.506 | Acc: 84.030% (8403/10000)\n",
            "\n",
            "Epoch: 56\n",
            "[================================================================>] Step: 0s | Tot: 10s | Loss: 0.284 | Acc: 90.410% (45205/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.377 | Acc: 88.130% (8813/10000)\n",
            "Saving..\n",
            "\n",
            "Epoch: 57\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.287 | Acc: 90.148% (45074/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.452 | Acc: 85.660% (8566/10000)\n",
            "\n",
            "Epoch: 58\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.286 | Acc: 90.240% (45120/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.447 | Acc: 85.180% (8518/10000)\n",
            "\n",
            "Epoch: 59\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.274 | Acc: 90.710% (45355/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.463 | Acc: 84.500% (8450/10000)\n",
            "\n",
            "Epoch: 60\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.276 | Acc: 90.556% (45278/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.401 | Acc: 87.030% (8703/10000)\n",
            "\n",
            "Epoch: 61\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.276 | Acc: 90.536% (45268/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.448 | Acc: 85.360% (8536/10000)\n",
            "\n",
            "Epoch: 62\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.270 | Acc: 90.674% (45337/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.422 | Acc: 85.800% (8580/10000)\n",
            "\n",
            "Epoch: 63\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.275 | Acc: 90.592% (45296/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.361 | Acc: 88.400% (8840/10000)\n",
            "Saving..\n",
            "\n",
            "Epoch: 64\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.273 | Acc: 90.852% (45426/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.354 | Acc: 87.920% (8792/10000)\n",
            "\n",
            "Epoch: 65\n",
            "[================================================================>] Step: 0s | Tot: 10s | Loss: 0.274 | Acc: 90.640% (45320/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.411 | Acc: 86.700% (8670/10000)\n",
            "\n",
            "Epoch: 66\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.268 | Acc: 90.816% (45408/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.343 | Acc: 88.900% (8890/10000)\n",
            "Saving..\n",
            "\n",
            "Epoch: 67\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.268 | Acc: 90.868% (45434/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.453 | Acc: 85.640% (8564/10000)\n",
            "\n",
            "Epoch: 68\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.266 | Acc: 90.938% (45469/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.366 | Acc: 87.360% (8736/10000)\n",
            "\n",
            "Epoch: 69\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.260 | Acc: 91.144% (45572/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.516 | Acc: 83.740% (8374/10000)\n",
            "\n",
            "Epoch: 70\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.255 | Acc: 91.320% (45660/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.419 | Acc: 86.790% (8679/10000)\n",
            "\n",
            "Epoch: 71\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.263 | Acc: 91.042% (45521/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.395 | Acc: 86.640% (8664/10000)\n",
            "\n",
            "Epoch: 72\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.256 | Acc: 91.330% (45665/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.334 | Acc: 89.300% (8930/10000)\n",
            "Saving..\n",
            "\n",
            "Epoch: 73\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.247 | Acc: 91.622% (45811/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.384 | Acc: 87.630% (8763/10000)\n",
            "\n",
            "Epoch: 74\n",
            "[================================================================>] Step: 0s | Tot: 10s | Loss: 0.253 | Acc: 91.312% (45656/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.377 | Acc: 87.870% (8787/10000)\n",
            "\n",
            "Epoch: 75\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.249 | Acc: 91.384% (45692/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.539 | Acc: 83.350% (8335/10000)\n",
            "\n",
            "Epoch: 76\n",
            "[================================================================>] Step: 0s | Tot: 10s | Loss: 0.243 | Acc: 91.746% (45873/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.408 | Acc: 86.680% (8668/10000)\n",
            "\n",
            "Epoch: 77\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.248 | Acc: 91.508% (45754/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.421 | Acc: 86.200% (8620/10000)\n",
            "\n",
            "Epoch: 78\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.240 | Acc: 91.792% (45896/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.420 | Acc: 86.330% (8633/10000)\n",
            "\n",
            "Epoch: 79\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.240 | Acc: 91.788% (45894/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.399 | Acc: 87.090% (8709/10000)\n",
            "\n",
            "Epoch: 80\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.235 | Acc: 92.068% (46034/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.410 | Acc: 86.860% (8686/10000)\n",
            "\n",
            "Epoch: 81\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.236 | Acc: 91.868% (45934/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.405 | Acc: 86.960% (8696/10000)\n",
            "\n",
            "Epoch: 82\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.232 | Acc: 92.316% (46158/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.457 | Acc: 85.610% (8561/10000)\n",
            "\n",
            "Epoch: 83\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.232 | Acc: 92.070% (46035/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.577 | Acc: 82.330% (8233/10000)\n",
            "\n",
            "Epoch: 84\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.230 | Acc: 92.142% (46071/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.339 | Acc: 88.490% (8849/10000)\n",
            "\n",
            "Epoch: 85\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.225 | Acc: 92.326% (46163/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.338 | Acc: 88.880% (8888/10000)\n",
            "\n",
            "Epoch: 86\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.225 | Acc: 92.276% (46138/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.396 | Acc: 86.910% (8691/10000)\n",
            "\n",
            "Epoch: 87\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.221 | Acc: 92.408% (46204/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.342 | Acc: 88.890% (8889/10000)\n",
            "\n",
            "Epoch: 88\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.219 | Acc: 92.500% (46250/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.344 | Acc: 89.100% (8910/10000)\n",
            "\n",
            "Epoch: 89\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.215 | Acc: 92.666% (46333/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.290 | Acc: 90.190% (9019/10000)\n",
            "Saving..\n",
            "\n",
            "Epoch: 90\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.210 | Acc: 92.922% (46461/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.414 | Acc: 86.760% (8676/10000)\n",
            "\n",
            "Epoch: 91\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.207 | Acc: 92.910% (46455/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.411 | Acc: 87.260% (8726/10000)\n",
            "\n",
            "Epoch: 92\n",
            "[================================================================>] Step: 0s | Tot: 10s | Loss: 0.210 | Acc: 92.786% (46393/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.364 | Acc: 88.090% (8809/10000)\n",
            "\n",
            "Epoch: 93\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.205 | Acc: 93.066% (46533/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.525 | Acc: 84.040% (8404/10000)\n",
            "\n",
            "Epoch: 94\n",
            "[================================================================>] Step: 0s | Tot: 10s | Loss: 0.203 | Acc: 93.150% (46575/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.307 | Acc: 89.910% (8991/10000)\n",
            "\n",
            "Epoch: 95\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.201 | Acc: 93.114% (46557/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.357 | Acc: 88.470% (8847/10000)\n",
            "\n",
            "Epoch: 96\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.200 | Acc: 93.226% (46613/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.353 | Acc: 88.650% (8865/10000)\n",
            "\n",
            "Epoch: 97\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.195 | Acc: 93.328% (46664/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.335 | Acc: 89.240% (8924/10000)\n",
            "\n",
            "Epoch: 98\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.192 | Acc: 93.424% (46712/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.384 | Acc: 87.920% (8792/10000)\n",
            "\n",
            "Epoch: 99\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.188 | Acc: 93.588% (46794/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.330 | Acc: 88.940% (8894/10000)\n",
            "\n",
            "Epoch: 100\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.184 | Acc: 93.852% (46926/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.436 | Acc: 86.640% (8664/10000)\n",
            "\n",
            "Epoch: 101\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.187 | Acc: 93.596% (46798/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.353 | Acc: 88.790% (8879/10000)\n",
            "\n",
            "Epoch: 102\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.181 | Acc: 93.768% (46884/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.383 | Acc: 88.160% (8816/10000)\n",
            "\n",
            "Epoch: 103\n",
            "[================================================================>] Step: 0s | Tot: 10s | Loss: 0.182 | Acc: 93.720% (46860/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.339 | Acc: 89.050% (8905/10000)\n",
            "\n",
            "Epoch: 104\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.177 | Acc: 93.888% (46944/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.334 | Acc: 89.270% (8927/10000)\n",
            "\n",
            "Epoch: 105\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.176 | Acc: 93.994% (46997/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.335 | Acc: 89.250% (8925/10000)\n",
            "\n",
            "Epoch: 106\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.174 | Acc: 94.014% (47007/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.325 | Acc: 89.500% (8950/10000)\n",
            "\n",
            "Epoch: 107\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.169 | Acc: 94.246% (47123/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.323 | Acc: 89.820% (8982/10000)\n",
            "\n",
            "Epoch: 108\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.161 | Acc: 94.460% (47230/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.317 | Acc: 90.180% (9018/10000)\n",
            "\n",
            "Epoch: 109\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.165 | Acc: 94.428% (47214/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.407 | Acc: 88.000% (8800/10000)\n",
            "\n",
            "Epoch: 110\n",
            "[================================================================>] Step: 0s | Tot: 10s | Loss: 0.158 | Acc: 94.594% (47297/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.365 | Acc: 88.860% (8886/10000)\n",
            "\n",
            "Epoch: 111\n",
            "[================================================================>] Step: 0s | Tot: 10s | Loss: 0.163 | Acc: 94.348% (47174/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.393 | Acc: 87.720% (8772/10000)\n",
            "\n",
            "Epoch: 112\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.150 | Acc: 94.882% (47441/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.327 | Acc: 89.730% (8973/10000)\n",
            "\n",
            "Epoch: 113\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.154 | Acc: 94.690% (47345/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.319 | Acc: 90.380% (9038/10000)\n",
            "Saving..\n",
            "\n",
            "Epoch: 114\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.143 | Acc: 95.052% (47526/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.305 | Acc: 90.420% (9042/10000)\n",
            "Saving..\n",
            "\n",
            "Epoch: 115\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.148 | Acc: 94.956% (47478/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.354 | Acc: 88.990% (8899/10000)\n",
            "\n",
            "Epoch: 116\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.145 | Acc: 95.044% (47522/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.318 | Acc: 90.020% (9002/10000)\n",
            "\n",
            "Epoch: 117\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.143 | Acc: 95.034% (47517/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.324 | Acc: 90.240% (9024/10000)\n",
            "\n",
            "Epoch: 118\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.136 | Acc: 95.376% (47688/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.300 | Acc: 90.520% (9052/10000)\n",
            "Saving..\n",
            "\n",
            "Epoch: 119\n",
            "[================================================================>] Step: 0s | Tot: 10s | Loss: 0.135 | Acc: 95.418% (47709/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.310 | Acc: 90.050% (9005/10000)\n",
            "\n",
            "Epoch: 120\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.128 | Acc: 95.582% (47791/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.302 | Acc: 90.870% (9087/10000)\n",
            "Saving..\n",
            "\n",
            "Epoch: 121\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.127 | Acc: 95.846% (47923/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.291 | Acc: 90.950% (9095/10000)\n",
            "Saving..\n",
            "\n",
            "Epoch: 122\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.126 | Acc: 95.654% (47827/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.291 | Acc: 91.130% (9113/10000)\n",
            "Saving..\n",
            "\n",
            "Epoch: 123\n",
            "[================================================================>] Step: 0s | Tot: 10s | Loss: 0.123 | Acc: 95.784% (47892/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.308 | Acc: 90.940% (9094/10000)\n",
            "\n",
            "Epoch: 124\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.125 | Acc: 95.754% (47877/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.341 | Acc: 89.610% (8961/10000)\n",
            "\n",
            "Epoch: 125\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.118 | Acc: 96.006% (48003/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.304 | Acc: 90.630% (9063/10000)\n",
            "\n",
            "Epoch: 126\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.111 | Acc: 96.206% (48103/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.308 | Acc: 90.540% (9054/10000)\n",
            "\n",
            "Epoch: 127\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.111 | Acc: 96.218% (48109/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.299 | Acc: 90.760% (9076/10000)\n",
            "\n",
            "Epoch: 128\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.108 | Acc: 96.294% (48147/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.338 | Acc: 89.840% (8984/10000)\n",
            "\n",
            "Epoch: 129\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.107 | Acc: 96.298% (48149/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.307 | Acc: 90.610% (9061/10000)\n",
            "\n",
            "Epoch: 130\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.098 | Acc: 96.720% (48360/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.298 | Acc: 91.090% (9109/10000)\n",
            "\n",
            "Epoch: 131\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.099 | Acc: 96.634% (48317/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.319 | Acc: 90.690% (9069/10000)\n",
            "\n",
            "Epoch: 132\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.096 | Acc: 96.702% (48351/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.283 | Acc: 91.470% (9147/10000)\n",
            "Saving..\n",
            "\n",
            "Epoch: 133\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.091 | Acc: 96.958% (48479/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.282 | Acc: 91.480% (9148/10000)\n",
            "Saving..\n",
            "\n",
            "Epoch: 134\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.088 | Acc: 96.972% (48486/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.295 | Acc: 91.430% (9143/10000)\n",
            "\n",
            "Epoch: 135\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.089 | Acc: 97.016% (48508/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.330 | Acc: 90.330% (9033/10000)\n",
            "\n",
            "Epoch: 136\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.084 | Acc: 97.134% (48567/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.268 | Acc: 92.220% (9222/10000)\n",
            "Saving..\n",
            "\n",
            "Epoch: 137\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.080 | Acc: 97.282% (48641/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.274 | Acc: 92.000% (9200/10000)\n",
            "\n",
            "Epoch: 138\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.083 | Acc: 97.236% (48618/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.323 | Acc: 90.830% (9083/10000)\n",
            "\n",
            "Epoch: 139\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.076 | Acc: 97.380% (48690/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.267 | Acc: 92.060% (9206/10000)\n",
            "\n",
            "Epoch: 140\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.073 | Acc: 97.472% (48736/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.258 | Acc: 92.630% (9263/10000)\n",
            "Saving..\n",
            "\n",
            "Epoch: 141\n",
            "[================================================================>] Step: 0s | Tot: 10s | Loss: 0.076 | Acc: 97.508% (48754/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.245 | Acc: 92.930% (9293/10000)\n",
            "Saving..\n",
            "\n",
            "Epoch: 142\n",
            "[================================================================>] Step: 0s | Tot: 10s | Loss: 0.065 | Acc: 97.814% (48907/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.263 | Acc: 92.500% (9250/10000)\n",
            "\n",
            "Epoch: 143\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.062 | Acc: 97.916% (48958/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.308 | Acc: 91.480% (9148/10000)\n",
            "\n",
            "Epoch: 144\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.064 | Acc: 97.908% (48954/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.262 | Acc: 92.280% (9228/10000)\n",
            "\n",
            "Epoch: 145\n",
            "[================================================================>] Step: 0s | Tot: 10s | Loss: 0.057 | Acc: 98.030% (49015/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.280 | Acc: 92.080% (9208/10000)\n",
            "\n",
            "Epoch: 146\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.055 | Acc: 98.198% (49099/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.245 | Acc: 92.920% (9292/10000)\n",
            "\n",
            "Epoch: 147\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.048 | Acc: 98.416% (49208/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.262 | Acc: 92.840% (9284/10000)\n",
            "\n",
            "Epoch: 148\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.051 | Acc: 98.380% (49190/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.280 | Acc: 92.190% (9219/10000)\n",
            "\n",
            "Epoch: 149\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.051 | Acc: 98.334% (49167/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.255 | Acc: 92.720% (9272/10000)\n",
            "\n",
            "Epoch: 150\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.042 | Acc: 98.674% (49337/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.266 | Acc: 92.700% (9270/10000)\n",
            "\n",
            "Epoch: 151\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.045 | Acc: 98.526% (49263/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.271 | Acc: 92.430% (9243/10000)\n",
            "\n",
            "Epoch: 152\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.039 | Acc: 98.742% (49371/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.250 | Acc: 93.130% (9313/10000)\n",
            "Saving..\n",
            "\n",
            "Epoch: 153\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.037 | Acc: 98.820% (49410/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.251 | Acc: 93.240% (9324/10000)\n",
            "Saving..\n",
            "\n",
            "Epoch: 154\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.030 | Acc: 99.074% (49537/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.246 | Acc: 93.370% (9337/10000)\n",
            "Saving..\n",
            "\n",
            "Epoch: 155\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.026 | Acc: 99.238% (49619/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.228 | Acc: 93.900% (9390/10000)\n",
            "Saving..\n",
            "\n",
            "Epoch: 156\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.026 | Acc: 99.180% (49590/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.262 | Acc: 92.950% (9295/10000)\n",
            "\n",
            "Epoch: 157\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.029 | Acc: 99.088% (49544/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.231 | Acc: 93.820% (9382/10000)\n",
            "\n",
            "Epoch: 158\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.025 | Acc: 99.254% (49627/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.244 | Acc: 93.490% (9349/10000)\n",
            "\n",
            "Epoch: 159\n",
            "[================================================================>] Step: 0s | Tot: 10s | Loss: 0.025 | Acc: 99.202% (49601/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.238 | Acc: 93.640% (9364/10000)\n",
            "\n",
            "Epoch: 160\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.017 | Acc: 99.488% (49744/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.230 | Acc: 93.870% (9387/10000)\n",
            "\n",
            "Epoch: 161\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.016 | Acc: 99.542% (49771/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.227 | Acc: 93.770% (9377/10000)\n",
            "\n",
            "Epoch: 162\n",
            "[================================================================>] Step: 0s | Tot: 10s | Loss: 0.015 | Acc: 99.556% (49778/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.218 | Acc: 94.110% (9411/10000)\n",
            "Saving..\n",
            "\n",
            "Epoch: 163\n",
            "[================================================================>] Step: 0s | Tot: 10s | Loss: 0.014 | Acc: 99.598% (49799/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.215 | Acc: 94.160% (9416/10000)\n",
            "Saving..\n",
            "\n",
            "Epoch: 164\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.010 | Acc: 99.740% (49870/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.227 | Acc: 94.250% (9425/10000)\n",
            "Saving..\n",
            "\n",
            "Epoch: 165\n",
            "[================================================================>] Step: 0s | Tot: 10s | Loss: 0.009 | Acc: 99.780% (49890/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.219 | Acc: 94.480% (9448/10000)\n",
            "Saving..\n",
            "\n",
            "Epoch: 166\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.008 | Acc: 99.788% (49894/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.206 | Acc: 94.790% (9479/10000)\n",
            "Saving..\n",
            "\n",
            "Epoch: 167\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.008 | Acc: 99.808% (49904/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.219 | Acc: 94.460% (9446/10000)\n",
            "\n",
            "Epoch: 168\n",
            "[================================================================>] Step: 0s | Tot: 10s | Loss: 0.005 | Acc: 99.884% (49942/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.206 | Acc: 94.750% (9475/10000)\n",
            "\n",
            "Epoch: 169\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.005 | Acc: 99.876% (49938/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.199 | Acc: 94.780% (9478/10000)\n",
            "\n",
            "Epoch: 170\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.004 | Acc: 99.926% (49963/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.189 | Acc: 95.190% (9519/10000)\n",
            "Saving..\n",
            "\n",
            "Epoch: 171\n",
            "[================================================================>] Step: 0s | Tot: 10s | Loss: 0.004 | Acc: 99.946% (49973/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.190 | Acc: 95.220% (9522/10000)\n",
            "Saving..\n",
            "\n",
            "Epoch: 172\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.003 | Acc: 99.954% (49977/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.187 | Acc: 95.110% (9511/10000)\n",
            "\n",
            "Epoch: 173\n",
            "[================================================================>] Step: 0s | Tot: 10s | Loss: 0.003 | Acc: 99.980% (49990/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.187 | Acc: 95.180% (9518/10000)\n",
            "\n",
            "Epoch: 174\n",
            "[================================================================>] Step: 0s | Tot: 10s | Loss: 0.003 | Acc: 99.964% (49982/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.185 | Acc: 95.170% (9517/10000)\n",
            "\n",
            "Epoch: 175\n",
            "[================================================================>] Step: 0s | Tot: 10s | Loss: 0.002 | Acc: 99.988% (49994/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.183 | Acc: 95.340% (9534/10000)\n",
            "Saving..\n",
            "\n",
            "Epoch: 176\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.002 | Acc: 99.984% (49992/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.185 | Acc: 95.230% (9523/10000)\n",
            "\n",
            "Epoch: 177\n",
            "[================================================================>] Step: 0s | Tot: 10s | Loss: 0.002 | Acc: 99.982% (49991/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.182 | Acc: 95.310% (9531/10000)\n",
            "\n",
            "Epoch: 178\n",
            "[================================================================>] Step: 0s | Tot: 10s | Loss: 0.002 | Acc: 99.990% (49995/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.176 | Acc: 95.520% (9552/10000)\n",
            "Saving..\n",
            "\n",
            "Epoch: 179\n",
            "[================================================================>] Step: 0s | Tot: 10s | Loss: 0.002 | Acc: 99.994% (49997/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.175 | Acc: 95.410% (9541/10000)\n",
            "\n",
            "Epoch: 180\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.002 | Acc: 99.990% (49995/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.177 | Acc: 95.410% (9541/10000)\n",
            "\n",
            "Epoch: 181\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.002 | Acc: 99.992% (49996/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.176 | Acc: 95.530% (9553/10000)\n",
            "Saving..\n",
            "\n",
            "Epoch: 182\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.002 | Acc: 99.996% (49998/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.174 | Acc: 95.510% (9551/10000)\n",
            "\n",
            "Epoch: 183\n",
            "[================================================================>] Step: 0s | Tot: 10s | Loss: 0.002 | Acc: 99.998% (49999/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.174 | Acc: 95.490% (9549/10000)\n",
            "\n",
            "Epoch: 184\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.002 | Acc: 99.990% (49995/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.175 | Acc: 95.490% (9549/10000)\n",
            "\n",
            "Epoch: 185\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.002 | Acc: 99.994% (49997/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.175 | Acc: 95.500% (9550/10000)\n",
            "\n",
            "Epoch: 186\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.002 | Acc: 99.990% (49995/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.175 | Acc: 95.470% (9547/10000)\n",
            "\n",
            "Epoch: 187\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.002 | Acc: 99.994% (49997/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.173 | Acc: 95.500% (9550/10000)\n",
            "\n",
            "Epoch: 188\n",
            "[================================================================>] Step: 0s | Tot: 10s | Loss: 0.002 | Acc: 99.998% (49999/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.175 | Acc: 95.400% (9540/10000)\n",
            "\n",
            "Epoch: 189\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.002 | Acc: 99.994% (49997/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.174 | Acc: 95.560% (9556/10000)\n",
            "Saving..\n",
            "\n",
            "Epoch: 190\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.002 | Acc: 99.994% (49997/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.172 | Acc: 95.500% (9550/10000)\n",
            "\n",
            "Epoch: 191\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.002 | Acc: 100.000% (50000/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.173 | Acc: 95.480% (9548/10000)\n",
            "\n",
            "Epoch: 192\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.002 | Acc: 99.996% (49998/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.174 | Acc: 95.440% (9544/10000)\n",
            "\n",
            "Epoch: 193\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.002 | Acc: 99.990% (49995/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.172 | Acc: 95.580% (9558/10000)\n",
            "Saving..\n",
            "\n",
            "Epoch: 194\n",
            "[================================================================>] Step: 0s | Tot: 10s | Loss: 0.002 | Acc: 99.996% (49998/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.173 | Acc: 95.550% (9555/10000)\n",
            "\n",
            "Epoch: 195\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.002 | Acc: 99.996% (49998/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.173 | Acc: 95.490% (9549/10000)\n",
            "\n",
            "Epoch: 196\n",
            "[================================================================>] Step: 0s | Tot: 10s | Loss: 0.002 | Acc: 100.000% (50000/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.172 | Acc: 95.550% (9555/10000)\n",
            "\n",
            "Epoch: 197\n",
            "[================================================================>] Step: 0s | Tot: 9s | Loss: 0.002 | Acc: 99.996% (49998/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.172 | Acc: 95.490% (9549/10000)\n",
            "\n",
            "Epoch: 198\n",
            "[================================================================>] Step: 0s | Tot: 10s | Loss: 0.002 | Acc: 100.000% (50000/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.172 | Acc: 95.540% (9554/10000)\n",
            "\n",
            "Epoch: 199\n",
            "[================================================================>] Step: 0s | Tot: 10s | Loss: 0.002 | Acc: 99.996% (49998/50000)\n",
            "[================================================================>] Step: 0s | Tot: 1s | Loss: 0.174 | Acc: 95.520% (9552/10000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, time, os\n",
        "from models import resnet  # kuangliu models/ 사용 가정\n",
        "\n",
        "# 1) 견고한 state_dict 로더 (net/model 키, module. 접두 처리)\n",
        "def load_state_dict_safely(model, ckpt):\n",
        "    # 1) state_dict 추출\n",
        "    if isinstance(ckpt, dict) and \"state_dict\" in ckpt:\n",
        "        state = ckpt[\"state_dict\"]\n",
        "    elif isinstance(ckpt, dict) and \"net\" in ckpt:\n",
        "        state = ckpt[\"net\"]\n",
        "    elif isinstance(ckpt, dict) and \"model\" in ckpt:\n",
        "        state = ckpt[\"model\"]\n",
        "    else:\n",
        "        # 마지막 수단: ckpt 자체가 state_dict라고 가정\n",
        "        state = ckpt\n",
        "\n",
        "    # 2) DataParallel 호환 (module. 접두사 제거)\n",
        "    from collections import OrderedDict\n",
        "    new_state = OrderedDict()\n",
        "    for k, v in state.items():\n",
        "        nk = k.replace(\"module.\", \"\", 1) if k.startswith(\"module.\") else k\n",
        "        new_state[nk] = v\n",
        "\n",
        "    # 3) 로드\n",
        "    model.load_state_dict(new_state, strict=True)\n",
        "    return model\n",
        "\n",
        "# 2) 파라미터/희소도/크기/지연 측정 유틸\n",
        "def count_params(model):\n",
        "    total = sum(p.numel() for p in model.parameters())\n",
        "    nnz   = sum((p != 0).sum().item() for p in model.parameters() if p.dtype.is_floating_point)\n",
        "    return total, nnz\n",
        "\n",
        "@torch.no_grad()\n",
        "def measure_latency(model, device=\"cuda\", img_size=32, bs=256, iters=50):\n",
        "    model.eval().to(device)\n",
        "    x = torch.randn(bs, 3, img_size, img_size, device=device)\n",
        "    # warmup\n",
        "    for _ in range(10): _ = model(x)\n",
        "    if device == \"cuda\": torch.cuda.synchronize()\n",
        "    t = 0.0\n",
        "    for _ in range(iters):\n",
        "        t0 = time.perf_counter()\n",
        "        _ = model(x)\n",
        "        if device == \"cuda\": torch.cuda.synchronize()\n",
        "        t += (time.perf_counter() - t0) / bs\n",
        "    return (t/iters) * 1000.0  # ms/img\n",
        "\n",
        "# 3) 실행: ResNet-18 모델 만들고 ckpt 로드 → 표 값 출력\n",
        "ckpt = torch.load(\"checkpoint/ResNet18.pth\", map_location=\"cpu\")   # ← 네 파일 경로\n",
        "model = resnet.ResNet18()                               # CIFAR-10용 ResNet-18\n",
        "\n",
        "# 로드\n",
        "model = load_state_dict_safely(model, ckpt)\n",
        "\n",
        "# 비교표 값 계산\n",
        "total, nnz = count_params(model)\n",
        "sparsity = 1 - nnz/total\n",
        "size_MB  = total * 4 / (1024**2)  # FP32 가정(4B/weight)\n",
        "lat_ms   = measure_latency(model, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "acc      = ckpt.get(\"acc\", None)\n",
        "\n",
        "# 출력 (보고서 표 형식)\n",
        "print(f\"{'model':10s} {'sparsity':>10s} {'acc(%)':>10s} {'params(M)':>12s} {'size(MB)':>10s} {'lat(ms/img)':>12s}\")\n",
        "print(\"-\"*70)\n",
        "print(f\"{'ResNet18':10s} {sparsity:10.3f} { (acc if acc is not None else 0):10.2f} \"\n",
        "      f\"{total/1e6:12.2f} {size_MB:10.2f} {lat_ms:12.3f}\")"
      ],
      "metadata": {
        "id": "kLh8KTz6OKaZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c157b25c-8764-4d66-957e-cb05a96b93a8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model        sparsity     acc(%)    params(M)   size(MB)  lat(ms/img)\n",
            "----------------------------------------------------------------------\n",
            "ResNet18        0.000      95.58        11.17      42.63        0.026\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils import prune\n",
        "\n",
        "def magnitude_prune_global(model, sparsity=0.9):\n",
        "    params_to_prune = []\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
        "            params_to_prune.append((m, \"weight\"))\n",
        "    prune.global_unstructured(\n",
        "        params_to_prune, pruning_method=prune.L1Unstructured, amount=sparsity\n",
        "    )\n",
        "    for m, _ in params_to_prune:\n",
        "        prune.remove(m, \"weight\")\n",
        "    return model"
      ],
      "metadata": {
        "id": "DYMfz4Mp3D2p"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_magnitude_once(ckpt_path, save_path, sparsity=0.9, seed=1, finetune_epochs=5, lr=1e-2):\n",
        "    \"\"\"\n",
        "    kuangliu/pytorch-cifar 형태의 체크포인트(`'net'`, `'acc'`, `'epoch'`)를 지원하는\n",
        "    Magnitude-based pruning 실행 함수.\n",
        "    \"\"\"\n",
        "    import torch\n",
        "    from torchvision import datasets, transforms\n",
        "    from torch.utils.data import DataLoader\n",
        "\n",
        "    print(f\"\\n[MAG] seed={seed}, sparsity={sparsity}\")\n",
        "\n",
        "    # 랜덤 시드 고정\n",
        "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # 데이터셋\n",
        "    mean, std = (0.4914,0.4822,0.4465), (0.2023,0.1994,0.2010)\n",
        "    tf_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean,std)\n",
        "    ])\n",
        "    tf_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean,std)\n",
        "    ])\n",
        "    tr = datasets.CIFAR10(\"./data\", train=True, download=True, transform=tf_train)\n",
        "    te = datasets.CIFAR10(\"./data\", train=False, download=True, transform=tf_test)\n",
        "    train_loader = DataLoader(tr, batch_size=256, shuffle=True, num_workers=2, pin_memory=True)\n",
        "    test_loader  = DataLoader(te, batch_size=512, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "    # 모델 생성 및 ckpt 로드\n",
        "    model = resnet.ResNet18().to(device)\n",
        "    ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
        "\n",
        "    # ✅ 'net' 또는 'model' 키 자동 감지\n",
        "    state = ckpt.get(\"net\", ckpt.get(\"model\", ckpt))\n",
        "    from collections import OrderedDict\n",
        "    new_state = OrderedDict()\n",
        "    for k, v in state.items():\n",
        "        nk = k.replace(\"module.\", \"\", 1) if k.startswith(\"module.\") else k\n",
        "        new_state[nk] = v\n",
        "    model.load_state_dict(new_state, strict=True)\n",
        "\n",
        "    # 프루닝 전 정확도\n",
        "    base_acc, _ = evaluate(model, test_loader, device)\n",
        "    print(f\"[BASELINE] acc={base_acc:.2f}%\")\n",
        "\n",
        "    # ① Magnitude Pruning\n",
        "    model = magnitude_prune_global(model, sparsity=sparsity)\n",
        "    total, nnz = count_params(model)\n",
        "    print(f\"[PRUNED] sparsity={1 - nnz/total:.4f}, params={total/1e6:.2f}M\")\n",
        "\n",
        "    # ② 파인튜닝 (짧게)\n",
        "    ce = nn.CrossEntropyLoss()\n",
        "    opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4, nesterov=True)\n",
        "    for ep in range(finetune_epochs):\n",
        "        model.train()\n",
        "        loss_sum = correct = total_ = 0\n",
        "        for x, y in train_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            logits = model(x)\n",
        "            loss = ce(logits, y)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            loss_sum += loss.item() * x.size(0)\n",
        "            correct += (logits.argmax(1) == y).sum().item()\n",
        "            total_ += y.numel()\n",
        "        tr_acc = 100 * correct / total_\n",
        "        te_acc, te_loss = evaluate(model, test_loader, device)\n",
        "        print(f\"[FT {ep+1}/{finetune_epochs}] train_acc={tr_acc:.2f}%  test_acc={te_acc:.2f}%\")\n",
        "\n",
        "    # ③ 최종 평가 및 저장\n",
        "    acc, _ = evaluate(model, test_loader, device)\n",
        "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "    torch.save({\n",
        "        \"net\": model.state_dict(),        # ✅ kuangliu 포맷 유지\n",
        "        \"acc\": float(acc),\n",
        "        \"base_acc\": float(base_acc),\n",
        "        \"sparsity\": float(sparsity),\n",
        "        \"seed\": int(seed),\n",
        "        \"params_total\": int(total),\n",
        "        \"params_nnz\": int(nnz),\n",
        "    }, save_path)\n",
        "    print(f\"[DONE] acc={acc:.2f}%  saved: {save_path}\")\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, device):\n",
        "    model.eval()\n",
        "    ce = nn.CrossEntropyLoss()\n",
        "    total_loss, correct, total = 0.0, 0, 0\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits = model(x)\n",
        "        total_loss += ce(logits, y).item() * x.size(0)\n",
        "        correct += (logits.argmax(1) == y).sum().item()\n",
        "        total += y.numel()\n",
        "    return 100 * correct / total, total_loss / total"
      ],
      "metadata": {
        "id": "d0I3tliT7RaZ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sparsities = [0.00, 0.20, 0.40, 0.60, 0.80, 0.90, 0.95, 0.98]\n",
        "seeds = [1, 2, 3]\n",
        "ckpt_path = \"checkpoint/ResNet18.pth\"\n",
        "\n",
        "os.makedirs(\"results/mag\", exist_ok=True)\n",
        "\n",
        "for sp in sparsities:\n",
        "    for s in seeds:\n",
        "        run_magnitude_once(\n",
        "            ckpt_path=ckpt_path,\n",
        "            save_path=f\"results/mag/resnet18_mag_s{s}_sp{sp}.pth\",\n",
        "            sparsity=sp,\n",
        "            seed=s,\n",
        "            finetune_epochs=5,   # 빠른 실험용\n",
        "            lr=1e-2\n",
        "        )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "na1rsGt67Y7x",
        "outputId": "dfa04bcd-fab1-4914-a9ab-7d66fcdb91f5"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[MAG] seed=1, sparsity=0.0\n",
            "[BASELINE] acc=95.58%\n",
            "[PRUNED] sparsity=0.0000, params=11.17M\n",
            "[FT 1/5] train_acc=100.00%  test_acc=95.48%\n",
            "[FT 2/5] train_acc=100.00%  test_acc=95.53%\n",
            "[FT 3/5] train_acc=100.00%  test_acc=95.55%\n",
            "[FT 4/5] train_acc=100.00%  test_acc=95.48%\n",
            "[FT 5/5] train_acc=100.00%  test_acc=95.44%\n",
            "[DONE] acc=95.44%  saved: results/mag/resnet18_mag_s1_sp0.0.pth\n",
            "\n",
            "[MAG] seed=2, sparsity=0.0\n",
            "[BASELINE] acc=95.58%\n",
            "[PRUNED] sparsity=0.0000, params=11.17M\n",
            "[FT 1/5] train_acc=100.00%  test_acc=95.53%\n",
            "[FT 2/5] train_acc=100.00%  test_acc=95.62%\n",
            "[FT 3/5] train_acc=100.00%  test_acc=95.61%\n",
            "[FT 4/5] train_acc=100.00%  test_acc=95.43%\n",
            "[FT 5/5] train_acc=100.00%  test_acc=95.50%\n",
            "[DONE] acc=95.50%  saved: results/mag/resnet18_mag_s2_sp0.0.pth\n",
            "\n",
            "[MAG] seed=3, sparsity=0.0\n",
            "[BASELINE] acc=95.58%\n",
            "[PRUNED] sparsity=0.0000, params=11.17M\n",
            "[FT 1/5] train_acc=100.00%  test_acc=95.52%\n",
            "[FT 2/5] train_acc=100.00%  test_acc=95.57%\n",
            "[FT 3/5] train_acc=100.00%  test_acc=95.58%\n",
            "[FT 4/5] train_acc=99.99%  test_acc=95.51%\n",
            "[FT 5/5] train_acc=100.00%  test_acc=95.41%\n",
            "[DONE] acc=95.41%  saved: results/mag/resnet18_mag_s3_sp0.0.pth\n",
            "\n",
            "[MAG] seed=1, sparsity=0.2\n",
            "[BASELINE] acc=95.58%\n",
            "[PRUNED] sparsity=0.1998, params=11.17M\n",
            "[FT 1/5] train_acc=100.00%  test_acc=95.47%\n",
            "[FT 2/5] train_acc=100.00%  test_acc=95.55%\n",
            "[FT 3/5] train_acc=100.00%  test_acc=95.53%\n",
            "[FT 4/5] train_acc=100.00%  test_acc=95.46%\n",
            "[FT 5/5] train_acc=100.00%  test_acc=95.48%\n",
            "[DONE] acc=95.48%  saved: results/mag/resnet18_mag_s1_sp0.2.pth\n",
            "\n",
            "[MAG] seed=2, sparsity=0.2\n",
            "[BASELINE] acc=95.58%\n",
            "[PRUNED] sparsity=0.1998, params=11.17M\n",
            "[FT 1/5] train_acc=100.00%  test_acc=95.54%\n",
            "[FT 2/5] train_acc=100.00%  test_acc=95.62%\n",
            "[FT 3/5] train_acc=100.00%  test_acc=95.58%\n",
            "[FT 4/5] train_acc=100.00%  test_acc=95.43%\n",
            "[FT 5/5] train_acc=100.00%  test_acc=95.48%\n",
            "[DONE] acc=95.48%  saved: results/mag/resnet18_mag_s2_sp0.2.pth\n",
            "\n",
            "[MAG] seed=3, sparsity=0.2\n",
            "[BASELINE] acc=95.58%\n",
            "[PRUNED] sparsity=0.1998, params=11.17M\n",
            "[FT 1/5] train_acc=100.00%  test_acc=95.53%\n",
            "[FT 2/5] train_acc=100.00%  test_acc=95.59%\n",
            "[FT 3/5] train_acc=100.00%  test_acc=95.59%\n",
            "[FT 4/5] train_acc=99.99%  test_acc=95.51%\n",
            "[FT 5/5] train_acc=100.00%  test_acc=95.40%\n",
            "[DONE] acc=95.40%  saved: results/mag/resnet18_mag_s3_sp0.2.pth\n",
            "\n",
            "[MAG] seed=1, sparsity=0.4\n",
            "[BASELINE] acc=95.58%\n",
            "[PRUNED] sparsity=0.3997, params=11.17M\n",
            "[FT 1/5] train_acc=100.00%  test_acc=95.46%\n",
            "[FT 2/5] train_acc=100.00%  test_acc=95.54%\n",
            "[FT 3/5] train_acc=100.00%  test_acc=95.55%\n",
            "[FT 4/5] train_acc=100.00%  test_acc=95.49%\n",
            "[FT 5/5] train_acc=100.00%  test_acc=95.49%\n",
            "[DONE] acc=95.49%  saved: results/mag/resnet18_mag_s1_sp0.4.pth\n",
            "\n",
            "[MAG] seed=2, sparsity=0.4\n",
            "[BASELINE] acc=95.58%\n",
            "[PRUNED] sparsity=0.3997, params=11.17M\n",
            "[FT 1/5] train_acc=100.00%  test_acc=95.56%\n",
            "[FT 2/5] train_acc=100.00%  test_acc=95.67%\n",
            "[FT 3/5] train_acc=100.00%  test_acc=95.60%\n",
            "[FT 4/5] train_acc=100.00%  test_acc=95.44%\n",
            "[FT 5/5] train_acc=100.00%  test_acc=95.55%\n",
            "[DONE] acc=95.55%  saved: results/mag/resnet18_mag_s2_sp0.4.pth\n",
            "\n",
            "[MAG] seed=3, sparsity=0.4\n",
            "[BASELINE] acc=95.58%\n",
            "[PRUNED] sparsity=0.3997, params=11.17M\n",
            "[FT 1/5] train_acc=100.00%  test_acc=95.50%\n",
            "[FT 2/5] train_acc=100.00%  test_acc=95.54%\n",
            "[FT 3/5] train_acc=100.00%  test_acc=95.60%\n",
            "[FT 4/5] train_acc=99.99%  test_acc=95.54%\n",
            "[FT 5/5] train_acc=100.00%  test_acc=95.47%\n",
            "[DONE] acc=95.47%  saved: results/mag/resnet18_mag_s3_sp0.4.pth\n",
            "\n",
            "[MAG] seed=1, sparsity=0.6\n",
            "[BASELINE] acc=95.58%\n",
            "[PRUNED] sparsity=0.5995, params=11.17M\n",
            "[FT 1/5] train_acc=100.00%  test_acc=95.44%\n",
            "[FT 2/5] train_acc=100.00%  test_acc=95.46%\n",
            "[FT 3/5] train_acc=100.00%  test_acc=95.45%\n",
            "[FT 4/5] train_acc=100.00%  test_acc=95.53%\n",
            "[FT 5/5] train_acc=100.00%  test_acc=95.49%\n",
            "[DONE] acc=95.49%  saved: results/mag/resnet18_mag_s1_sp0.6.pth\n",
            "\n",
            "[MAG] seed=2, sparsity=0.6\n",
            "[BASELINE] acc=95.58%\n",
            "[PRUNED] sparsity=0.5995, params=11.17M\n",
            "[FT 1/5] train_acc=100.00%  test_acc=95.52%\n",
            "[FT 2/5] train_acc=100.00%  test_acc=95.54%\n",
            "[FT 3/5] train_acc=100.00%  test_acc=95.52%\n",
            "[FT 4/5] train_acc=100.00%  test_acc=95.46%\n",
            "[FT 5/5] train_acc=100.00%  test_acc=95.63%\n",
            "[DONE] acc=95.63%  saved: results/mag/resnet18_mag_s2_sp0.6.pth\n",
            "\n",
            "[MAG] seed=3, sparsity=0.6\n",
            "[BASELINE] acc=95.58%\n",
            "[PRUNED] sparsity=0.5995, params=11.17M\n",
            "[FT 1/5] train_acc=100.00%  test_acc=95.42%\n",
            "[FT 2/5] train_acc=100.00%  test_acc=95.46%\n",
            "[FT 3/5] train_acc=100.00%  test_acc=95.46%\n",
            "[FT 4/5] train_acc=99.99%  test_acc=95.48%\n",
            "[FT 5/5] train_acc=99.99%  test_acc=95.55%\n",
            "[DONE] acc=95.55%  saved: results/mag/resnet18_mag_s3_sp0.6.pth\n",
            "\n",
            "[MAG] seed=1, sparsity=0.8\n",
            "[BASELINE] acc=95.58%\n",
            "[PRUNED] sparsity=0.7993, params=11.17M\n",
            "[FT 1/5] train_acc=99.99%  test_acc=95.33%\n",
            "[FT 2/5] train_acc=99.99%  test_acc=95.33%\n",
            "[FT 3/5] train_acc=99.98%  test_acc=95.33%\n",
            "[FT 4/5] train_acc=99.99%  test_acc=95.13%\n",
            "[FT 5/5] train_acc=99.99%  test_acc=95.21%\n",
            "[DONE] acc=95.21%  saved: results/mag/resnet18_mag_s1_sp0.8.pth\n",
            "\n",
            "[MAG] seed=2, sparsity=0.8\n",
            "[BASELINE] acc=95.58%\n",
            "[PRUNED] sparsity=0.7993, params=11.17M\n",
            "[FT 1/5] train_acc=99.97%  test_acc=95.16%\n",
            "[FT 2/5] train_acc=99.98%  test_acc=95.01%\n",
            "[FT 3/5] train_acc=99.98%  test_acc=95.16%\n",
            "[FT 4/5] train_acc=99.99%  test_acc=95.28%\n",
            "[FT 5/5] train_acc=100.00%  test_acc=95.41%\n",
            "[DONE] acc=95.41%  saved: results/mag/resnet18_mag_s2_sp0.8.pth\n",
            "\n",
            "[MAG] seed=3, sparsity=0.8\n",
            "[BASELINE] acc=95.58%\n",
            "[PRUNED] sparsity=0.7993, params=11.17M\n",
            "[FT 1/5] train_acc=99.99%  test_acc=95.19%\n",
            "[FT 2/5] train_acc=99.99%  test_acc=95.36%\n",
            "[FT 3/5] train_acc=99.99%  test_acc=95.17%\n",
            "[FT 4/5] train_acc=99.98%  test_acc=95.19%\n",
            "[FT 5/5] train_acc=99.97%  test_acc=95.18%\n",
            "[DONE] acc=95.18%  saved: results/mag/resnet18_mag_s3_sp0.8.pth\n",
            "\n",
            "[MAG] seed=1, sparsity=0.9\n",
            "[BASELINE] acc=95.58%\n",
            "[PRUNED] sparsity=0.8992, params=11.17M\n",
            "[FT 1/5] train_acc=99.45%  test_acc=92.19%\n",
            "[FT 2/5] train_acc=98.89%  test_acc=92.68%\n",
            "[FT 3/5] train_acc=98.88%  test_acc=92.63%\n",
            "[FT 4/5] train_acc=98.96%  test_acc=92.81%\n",
            "[FT 5/5] train_acc=98.94%  test_acc=92.90%\n",
            "[DONE] acc=92.90%  saved: results/mag/resnet18_mag_s1_sp0.9.pth\n",
            "\n",
            "[MAG] seed=2, sparsity=0.9\n",
            "[BASELINE] acc=95.58%\n",
            "[PRUNED] sparsity=0.8992, params=11.17M\n",
            "[FT 1/5] train_acc=99.40%  test_acc=93.44%\n",
            "[FT 2/5] train_acc=98.85%  test_acc=91.88%\n",
            "[FT 3/5] train_acc=98.68%  test_acc=92.79%\n",
            "[FT 4/5] train_acc=98.63%  test_acc=92.32%\n",
            "[FT 5/5] train_acc=98.84%  test_acc=93.18%\n",
            "[DONE] acc=93.18%  saved: results/mag/resnet18_mag_s2_sp0.9.pth\n",
            "\n",
            "[MAG] seed=3, sparsity=0.9\n",
            "[BASELINE] acc=95.58%\n",
            "[PRUNED] sparsity=0.8992, params=11.17M\n",
            "[FT 1/5] train_acc=99.29%  test_acc=93.20%\n",
            "[FT 2/5] train_acc=98.95%  test_acc=92.94%\n",
            "[FT 3/5] train_acc=98.63%  test_acc=92.24%\n",
            "[FT 4/5] train_acc=98.94%  test_acc=92.64%\n",
            "[FT 5/5] train_acc=98.85%  test_acc=92.95%\n",
            "[DONE] acc=92.95%  saved: results/mag/resnet18_mag_s3_sp0.9.pth\n",
            "\n",
            "[MAG] seed=1, sparsity=0.95\n",
            "[BASELINE] acc=95.58%\n",
            "[PRUNED] sparsity=0.9492, params=11.17M\n",
            "[FT 1/5] train_acc=95.76%  test_acc=91.27%\n",
            "[FT 2/5] train_acc=96.91%  test_acc=91.73%\n",
            "[FT 3/5] train_acc=97.37%  test_acc=92.27%\n",
            "[FT 4/5] train_acc=97.77%  test_acc=91.70%\n",
            "[FT 5/5] train_acc=97.91%  test_acc=92.05%\n",
            "[DONE] acc=92.05%  saved: results/mag/resnet18_mag_s1_sp0.95.pth\n",
            "\n",
            "[MAG] seed=2, sparsity=0.95\n",
            "[BASELINE] acc=95.58%\n",
            "[PRUNED] sparsity=0.9492, params=11.17M\n",
            "[FT 1/5] train_acc=95.92%  test_acc=91.25%\n",
            "[FT 2/5] train_acc=96.75%  test_acc=91.50%\n",
            "[FT 3/5] train_acc=97.39%  test_acc=91.79%\n",
            "[FT 4/5] train_acc=97.79%  test_acc=91.93%\n",
            "[FT 5/5] train_acc=97.89%  test_acc=91.20%\n",
            "[DONE] acc=91.20%  saved: results/mag/resnet18_mag_s2_sp0.95.pth\n",
            "\n",
            "[MAG] seed=3, sparsity=0.95\n",
            "[BASELINE] acc=95.58%\n",
            "[PRUNED] sparsity=0.9492, params=11.17M\n",
            "[FT 1/5] train_acc=95.99%  test_acc=91.02%\n",
            "[FT 2/5] train_acc=96.80%  test_acc=91.14%\n",
            "[FT 3/5] train_acc=97.40%  test_acc=92.04%\n",
            "[FT 4/5] train_acc=97.93%  test_acc=92.47%\n",
            "[FT 5/5] train_acc=97.86%  test_acc=92.11%\n",
            "[DONE] acc=92.11%  saved: results/mag/resnet18_mag_s3_sp0.95.pth\n",
            "\n",
            "[MAG] seed=1, sparsity=0.98\n",
            "[BASELINE] acc=95.58%\n",
            "[PRUNED] sparsity=0.9792, params=11.17M\n",
            "[FT 1/5] train_acc=88.82%  test_acc=86.85%\n",
            "[FT 2/5] train_acc=94.18%  test_acc=91.16%\n",
            "[FT 3/5] train_acc=95.34%  test_acc=91.01%\n",
            "[FT 4/5] train_acc=96.07%  test_acc=91.69%\n",
            "[FT 5/5] train_acc=96.58%  test_acc=91.14%\n",
            "[DONE] acc=91.14%  saved: results/mag/resnet18_mag_s1_sp0.98.pth\n",
            "\n",
            "[MAG] seed=2, sparsity=0.98\n",
            "[BASELINE] acc=95.58%\n",
            "[PRUNED] sparsity=0.9792, params=11.17M\n",
            "[FT 1/5] train_acc=88.90%  test_acc=88.10%\n",
            "[FT 2/5] train_acc=93.92%  test_acc=89.91%\n",
            "[FT 3/5] train_acc=95.29%  test_acc=90.81%\n",
            "[FT 4/5] train_acc=96.11%  test_acc=91.40%\n",
            "[FT 5/5] train_acc=96.45%  test_acc=90.70%\n",
            "[DONE] acc=90.70%  saved: results/mag/resnet18_mag_s2_sp0.98.pth\n",
            "\n",
            "[MAG] seed=3, sparsity=0.98\n",
            "[BASELINE] acc=95.58%\n",
            "[PRUNED] sparsity=0.9792, params=11.17M\n",
            "[FT 1/5] train_acc=89.22%  test_acc=86.88%\n",
            "[FT 2/5] train_acc=93.97%  test_acc=90.43%\n",
            "[FT 3/5] train_acc=95.14%  test_acc=91.15%\n",
            "[FT 4/5] train_acc=96.12%  test_acc=90.44%\n",
            "[FT 5/5] train_acc=96.49%  test_acc=91.68%\n",
            "[DONE] acc=91.68%  saved: results/mag/resnet18_mag_s3_sp0.98.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------\n",
        "# OBD 핵심: H_ii 대각 근사(E[g^2]) 계산\n",
        "# --------------------------\n",
        "@torch.no_grad()\n",
        "def _is_prunable(m):\n",
        "    return isinstance(m, (nn.Conv2d, nn.Linear))\n",
        "\n",
        "def estimate_hessian_diag_eg2(model, loader, device, max_batches=100):\n",
        "    \"\"\"\n",
        "    E[g^2]로 H의 대각 근사 추정 (OBD에서 saliency = 0.5 * H_ii * w_i^2)\n",
        "    Conv/Linear의 weight만 대상으로 함.\n",
        "    \"\"\"\n",
        "    model.train()  # BN 통계 안정\n",
        "    ce = nn.CrossEntropyLoss()\n",
        "\n",
        "    # 대상 파라미터 수집 (Conv/Linear weight만)\n",
        "    modules, params = [], []\n",
        "    for m in model.modules():\n",
        "        if _is_prunable(m) and m.weight.requires_grad:\n",
        "            modules.append(m)\n",
        "            params.append(m.weight)\n",
        "\n",
        "    # 누적 버퍼\n",
        "    hdiag = [torch.zeros_like(p, device=device) for p in params]\n",
        "    batches = 0\n",
        "\n",
        "    for x,y in loader:\n",
        "        x,y = x.to(device), y.to(device)\n",
        "        model.zero_grad(set_to_none=True)\n",
        "        logits = model(x)\n",
        "        loss = ce(logits, y)\n",
        "\n",
        "        # g = dL/dw  (대상 파라미터만 미분)\n",
        "        grads = torch.autograd.grad(loss, params, retain_graph=False, create_graph=False, allow_unused=False)\n",
        "        for hd, g in zip(hdiag, grads):\n",
        "            hd += (g.detach() ** 2)\n",
        "        batches += 1\n",
        "        if batches >= max_batches:\n",
        "            break\n",
        "\n",
        "    for i in range(len(hdiag)):\n",
        "        hdiag[i] /= max(1, batches)\n",
        "    return hdiag, modules  # modules[i].weight와 hdiag[i]가 1:1"
      ],
      "metadata": {
        "id": "krxqpJtB7gKd"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------\n",
        "# OBD 프루닝 (글로벌)\n",
        "# --------------------------\n",
        "def obd_prune_global(model, loader, device, sparsity=0.9, max_batches=100):\n",
        "    \"\"\"\n",
        "    saliency_i = 0.5 * H_ii * w_i^2 를 전 레이어에서 모아\n",
        "    작은 것부터 amount(=sparsity) 비율만큼 0으로 설정.\n",
        "    \"\"\"\n",
        "    hdiag_list, modules = estimate_hessian_diag_eg2(model, loader, device, max_batches=max_batches)\n",
        "\n",
        "    # 레이어별 saliency 계산 & 전역 임계값 산출\n",
        "    saliency_flat = []\n",
        "    for m, Hd in zip(modules, hdiag_list):\n",
        "        W = m.weight.data\n",
        "        sal = 0.5 * Hd * (W ** 2)\n",
        "        saliency_flat.append(sal.flatten())\n",
        "    saliency_flat = torch.cat(saliency_flat)\n",
        "\n",
        "    k = int(saliency_flat.numel() * sparsity)\n",
        "    if k > 0:\n",
        "        thresh = torch.topk(saliency_flat, k, largest=False).values.max()\n",
        "    else:\n",
        "        thresh = saliency_flat.min() - 1  # 아무것도 자르지 않음\n",
        "\n",
        "    # 임계값 이하를 0으로\n",
        "    for m, Hd in zip(modules, hdiag_list):\n",
        "        W = m.weight.data\n",
        "        sal = 0.5 * Hd * (W ** 2)\n",
        "        mask = (sal <= thresh)\n",
        "        W[mask] = 0.0\n",
        "\n",
        "    return model\n",
        "\n",
        "# --------------------------\n",
        "# 통합 실행 함수: ckpt 로드 → OBD → (짧게) 파인튜닝 → 저장\n",
        "# --------------------------\n",
        "def run_obd_once(ckpt_path, save_path, sparsity=0.9, seed=1, finetune_epochs=5, lr=1e-2, max_batches=100):\n",
        "    \"\"\"\n",
        "    kuangliu/pytorch-cifar 형태('net' 키)의 ckpt 지원.\n",
        "    \"\"\"\n",
        "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # 데이터\n",
        "    train_loader, test_loader = get_loaders(batch_train=256, batch_test=512, workers=2)\n",
        "\n",
        "    # 모델 & ckpt 로드\n",
        "    model = resnet.ResNet18().to(device)\n",
        "    ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
        "    state = ckpt.get(\"net\", ckpt.get(\"model\", ckpt))\n",
        "    new_state = OrderedDict((k.replace(\"module.\",\"\",1) if k.startswith(\"module.\") else k, v) for k,v in state.items())\n",
        "    model.load_state_dict(new_state, strict=True)\n",
        "\n",
        "    # 프루닝 전 성능\n",
        "    base_acc, _ = evaluate(model, test_loader, device)\n",
        "    print(f\"[BASELINE] acc={base_acc:.2f}%\")\n",
        "\n",
        "    # OBD 프루닝\n",
        "    model = obd_prune_global(model, train_loader, device, sparsity=sparsity, max_batches=max_batches)\n",
        "    total, nnz = count_params(model)\n",
        "    print(f\"[OBD-PRUNED] sparsity={1 - nnz/total:.4f}, params={total/1e6:.2f}M\")\n",
        "\n",
        "    # 짧은 파인튜닝\n",
        "    if finetune_epochs > 0:\n",
        "        ce = nn.CrossEntropyLoss()\n",
        "        opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4, nesterov=True)\n",
        "        for ep in range(finetune_epochs):\n",
        "            model.train()\n",
        "            loss_sum = cor = tot = 0\n",
        "            for x,y in train_loader:\n",
        "                x,y = x.to(device), y.to(device)\n",
        "                opt.zero_grad(set_to_none=True)\n",
        "                logits = model(x); loss = ce(logits,y)\n",
        "                loss.backward(); opt.step()\n",
        "                loss_sum += loss.item()*x.size(0)\n",
        "                cor += (logits.argmax(1)==y).sum().item()\n",
        "                tot += y.numel()\n",
        "            tr_acc = 100*cor/tot\n",
        "            te_acc, te_loss = evaluate(model, test_loader, device)\n",
        "            print(f\"[FT {ep+1}/{finetune_epochs}] train_acc={tr_acc:.2f}%  test_acc={te_acc:.2f}%\")\n",
        "\n",
        "    # 저장(kuangliu 포맷)\n",
        "    acc, _ = evaluate(model, test_loader, device)\n",
        "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "    torch.save({\n",
        "        \"net\": model.state_dict(),\n",
        "        \"acc\": float(acc),\n",
        "        \"base_acc\": float(base_acc),\n",
        "        \"sparsity\": float(sparsity),\n",
        "        \"seed\": int(seed),\n",
        "        \"params_total\": int(total),\n",
        "        \"params_nnz\": int(nnz),\n",
        "        \"method\": \"obd\",\n",
        "    }, save_path)\n",
        "    print(f\"[DONE][OBD] sp={sparsity} acc={acc:.2f}% → {save_path}\")"
      ],
      "metadata": {
        "id": "hicd2cq9EPCJ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_loaders(batch_train=256, batch_test=512, workers=2):\n",
        "    mean,std=(0.4914,0.4822,0.4465),(0.2023,0.1994,0.2010)\n",
        "    tf_tr = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean,std),\n",
        "    ])\n",
        "    tf_te = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean,std)])\n",
        "    tr = torchvision.datasets.CIFAR10(\"./data\", train=True, download=True, transform=tf_tr)\n",
        "    te = torchvision.datasets.CIFAR10(\"./data\", train=False, download=True, transform=tf_te)\n",
        "    tr_loader = torch.utils.data.DataLoader(tr, batch_size=batch_train, shuffle=True, num_workers=workers, pin_memory=True, drop_last=True)\n",
        "    te_loader = torch.utils.data.DataLoader(te, batch_size=batch_test, shuffle=False, num_workers=workers, pin_memory=True)\n",
        "    return tr_loader, te_loader"
      ],
      "metadata": {
        "id": "2b5BidvoFAOw"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict\n",
        "\n",
        "ckpt_path = \"checkpoint/ResNet18.pth\"  # 너의 베이스라인 ckpt 경로\n",
        "os.makedirs(\"results/obd\", exist_ok=True)\n",
        "\n",
        "sparsities = [0.00, 0.20, 0.40, 0.60, 0.80, 0.90, 0.95, 0.98]\n",
        "seeds = [1, 2, 3]\n",
        "\n",
        "for sp in sparsities:\n",
        "    for s in seeds:\n",
        "        run_obd_once(\n",
        "            ckpt_path=ckpt_path,\n",
        "            save_path=f\"results/obd/resnet18_obd_s{s}_sp{sp}.pth\",\n",
        "            sparsity=sp, seed=s,\n",
        "            finetune_epochs=5, lr=1e-2,\n",
        "            max_batches=200,\n",
        "        )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "erNLCcJaEVng",
        "outputId": "2a3ad48e-8b35-4456-88e1-a06575b633df"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[BASELINE] acc=95.58%\n",
            "[OBD-PRUNED] sparsity=0.0000, params=11.17M\n",
            "[FT 1/5] train_acc=99.99%  test_acc=95.62%\n",
            "[FT 2/5] train_acc=99.99%  test_acc=95.50%\n",
            "[FT 3/5] train_acc=100.00%  test_acc=95.52%\n",
            "[FT 4/5] train_acc=99.99%  test_acc=95.50%\n",
            "[FT 5/5] train_acc=100.00%  test_acc=95.34%\n",
            "[DONE][OBD] sp=0.0 acc=95.34% → results/obd/resnet18_obd_s1_sp0.0.pth\n",
            "[BASELINE] acc=95.58%\n",
            "[OBD-PRUNED] sparsity=0.0000, params=11.17M\n",
            "[FT 1/5] train_acc=100.00%  test_acc=95.54%\n",
            "[FT 2/5] train_acc=100.00%  test_acc=95.45%\n",
            "[FT 3/5] train_acc=99.99%  test_acc=95.36%\n",
            "[FT 4/5] train_acc=100.00%  test_acc=95.53%\n",
            "[FT 5/5] train_acc=100.00%  test_acc=95.64%\n",
            "[DONE][OBD] sp=0.0 acc=95.64% → results/obd/resnet18_obd_s2_sp0.0.pth\n",
            "[BASELINE] acc=95.58%\n",
            "[OBD-PRUNED] sparsity=0.0000, params=11.17M\n",
            "[FT 1/5] train_acc=100.00%  test_acc=95.49%\n",
            "[FT 2/5] train_acc=100.00%  test_acc=95.51%\n",
            "[FT 3/5] train_acc=100.00%  test_acc=95.46%\n",
            "[FT 4/5] train_acc=100.00%  test_acc=95.46%\n",
            "[FT 5/5] train_acc=100.00%  test_acc=95.47%\n",
            "[DONE][OBD] sp=0.0 acc=95.47% → results/obd/resnet18_obd_s3_sp0.0.pth\n",
            "[BASELINE] acc=95.58%\n",
            "[OBD-PRUNED] sparsity=0.1998, params=11.17M\n",
            "[FT 1/5] train_acc=100.00%  test_acc=95.61%\n",
            "[FT 2/5] train_acc=99.99%  test_acc=95.46%\n",
            "[FT 3/5] train_acc=100.00%  test_acc=95.53%\n",
            "[FT 4/5] train_acc=99.99%  test_acc=95.49%\n",
            "[FT 5/5] train_acc=100.00%  test_acc=95.33%\n",
            "[DONE][OBD] sp=0.2 acc=95.33% → results/obd/resnet18_obd_s1_sp0.2.pth\n",
            "[BASELINE] acc=95.58%\n",
            "[OBD-PRUNED] sparsity=0.1998, params=11.17M\n",
            "[FT 1/5] train_acc=100.00%  test_acc=95.55%\n",
            "[FT 2/5] train_acc=100.00%  test_acc=95.46%\n",
            "[FT 3/5] train_acc=99.99%  test_acc=95.36%\n",
            "[FT 4/5] train_acc=100.00%  test_acc=95.57%\n",
            "[FT 5/5] train_acc=100.00%  test_acc=95.62%\n",
            "[DONE][OBD] sp=0.2 acc=95.62% → results/obd/resnet18_obd_s2_sp0.2.pth\n",
            "[BASELINE] acc=95.58%\n",
            "[OBD-PRUNED] sparsity=0.1998, params=11.17M\n",
            "[FT 1/5] train_acc=100.00%  test_acc=95.53%\n",
            "[FT 2/5] train_acc=100.00%  test_acc=95.49%\n",
            "[FT 3/5] train_acc=100.00%  test_acc=95.50%\n",
            "[FT 4/5] train_acc=100.00%  test_acc=95.46%\n",
            "[FT 5/5] train_acc=100.00%  test_acc=95.46%\n",
            "[DONE][OBD] sp=0.2 acc=95.46% → results/obd/resnet18_obd_s3_sp0.2.pth\n",
            "[BASELINE] acc=95.58%\n",
            "[OBD-PRUNED] sparsity=0.3997, params=11.17M\n",
            "[FT 1/5] train_acc=100.00%  test_acc=95.59%\n",
            "[FT 2/5] train_acc=100.00%  test_acc=95.46%\n",
            "[FT 3/5] train_acc=100.00%  test_acc=95.53%\n",
            "[FT 4/5] train_acc=99.99%  test_acc=95.40%\n",
            "[FT 5/5] train_acc=99.99%  test_acc=95.34%\n",
            "[DONE][OBD] sp=0.4 acc=95.34% → results/obd/resnet18_obd_s1_sp0.4.pth\n",
            "[BASELINE] acc=95.58%\n",
            "[OBD-PRUNED] sparsity=0.3997, params=11.17M\n",
            "[FT 1/5] train_acc=100.00%  test_acc=95.50%\n",
            "[FT 2/5] train_acc=100.00%  test_acc=95.55%\n",
            "[FT 3/5] train_acc=100.00%  test_acc=95.43%\n",
            "[FT 4/5] train_acc=100.00%  test_acc=95.55%\n",
            "[FT 5/5] train_acc=100.00%  test_acc=95.60%\n",
            "[DONE][OBD] sp=0.4 acc=95.60% → results/obd/resnet18_obd_s2_sp0.4.pth\n",
            "[BASELINE] acc=95.58%\n",
            "[OBD-PRUNED] sparsity=0.3997, params=11.17M\n",
            "[FT 1/5] train_acc=100.00%  test_acc=95.46%\n",
            "[FT 2/5] train_acc=100.00%  test_acc=95.51%\n",
            "[FT 3/5] train_acc=100.00%  test_acc=95.53%\n",
            "[FT 4/5] train_acc=100.00%  test_acc=95.47%\n",
            "[FT 5/5] train_acc=100.00%  test_acc=95.48%\n",
            "[DONE][OBD] sp=0.4 acc=95.48% → results/obd/resnet18_obd_s3_sp0.4.pth\n",
            "[BASELINE] acc=95.58%\n",
            "[OBD-PRUNED] sparsity=0.5995, params=11.17M\n",
            "[FT 1/5] train_acc=99.99%  test_acc=95.37%\n",
            "[FT 2/5] train_acc=99.99%  test_acc=95.40%\n",
            "[FT 3/5] train_acc=100.00%  test_acc=95.48%\n",
            "[FT 4/5] train_acc=100.00%  test_acc=95.56%\n",
            "[FT 5/5] train_acc=99.99%  test_acc=95.42%\n",
            "[DONE][OBD] sp=0.6 acc=95.42% → results/obd/resnet18_obd_s1_sp0.6.pth\n",
            "[BASELINE] acc=95.58%\n",
            "[OBD-PRUNED] sparsity=0.5995, params=11.17M\n",
            "[FT 1/5] train_acc=100.00%  test_acc=95.49%\n",
            "[FT 2/5] train_acc=100.00%  test_acc=95.48%\n",
            "[FT 3/5] train_acc=100.00%  test_acc=95.46%\n",
            "[FT 4/5] train_acc=100.00%  test_acc=95.49%\n",
            "[FT 5/5] train_acc=100.00%  test_acc=95.62%\n",
            "[DONE][OBD] sp=0.6 acc=95.62% → results/obd/resnet18_obd_s2_sp0.6.pth\n",
            "[BASELINE] acc=95.58%\n",
            "[OBD-PRUNED] sparsity=0.5995, params=11.17M\n",
            "[FT 1/5] train_acc=99.99%  test_acc=95.34%\n",
            "[FT 2/5] train_acc=100.00%  test_acc=95.41%\n",
            "[FT 3/5] train_acc=100.00%  test_acc=95.46%\n",
            "[FT 4/5] train_acc=99.99%  test_acc=95.37%\n",
            "[FT 5/5] train_acc=100.00%  test_acc=95.38%\n",
            "[DONE][OBD] sp=0.6 acc=95.38% → results/obd/resnet18_obd_s3_sp0.6.pth\n",
            "[BASELINE] acc=95.58%\n",
            "[OBD-PRUNED] sparsity=0.7993, params=11.17M\n",
            "[FT 1/5] train_acc=99.79%  test_acc=94.75%\n",
            "[FT 2/5] train_acc=99.84%  test_acc=94.77%\n",
            "[FT 3/5] train_acc=99.84%  test_acc=94.41%\n",
            "[FT 4/5] train_acc=99.72%  test_acc=94.08%\n",
            "[FT 5/5] train_acc=99.54%  test_acc=93.84%\n",
            "[DONE][OBD] sp=0.8 acc=93.84% → results/obd/resnet18_obd_s1_sp0.8.pth\n",
            "[BASELINE] acc=95.58%\n",
            "[OBD-PRUNED] sparsity=0.7993, params=11.17M\n",
            "[FT 1/5] train_acc=99.80%  test_acc=94.75%\n",
            "[FT 2/5] train_acc=99.83%  test_acc=94.35%\n",
            "[FT 3/5] train_acc=99.78%  test_acc=94.17%\n",
            "[FT 4/5] train_acc=99.67%  test_acc=94.02%\n",
            "[FT 5/5] train_acc=99.61%  test_acc=93.07%\n",
            "[DONE][OBD] sp=0.8 acc=93.07% → results/obd/resnet18_obd_s2_sp0.8.pth\n",
            "[BASELINE] acc=95.58%\n",
            "[OBD-PRUNED] sparsity=0.7993, params=11.17M\n",
            "[FT 1/5] train_acc=99.80%  test_acc=94.72%\n",
            "[FT 2/5] train_acc=99.84%  test_acc=94.60%\n",
            "[FT 3/5] train_acc=99.75%  test_acc=93.93%\n",
            "[FT 4/5] train_acc=99.71%  test_acc=93.96%\n",
            "[FT 5/5] train_acc=99.49%  test_acc=93.37%\n",
            "[DONE][OBD] sp=0.8 acc=93.37% → results/obd/resnet18_obd_s3_sp0.8.pth\n",
            "[BASELINE] acc=95.58%\n",
            "[OBD-PRUNED] sparsity=0.8992, params=11.17M\n",
            "[FT 1/5] train_acc=97.71%  test_acc=92.38%\n",
            "[FT 2/5] train_acc=98.32%  test_acc=92.49%\n",
            "[FT 3/5] train_acc=98.34%  test_acc=92.53%\n",
            "[FT 4/5] train_acc=98.45%  test_acc=92.61%\n",
            "[FT 5/5] train_acc=98.59%  test_acc=92.09%\n",
            "[DONE][OBD] sp=0.9 acc=92.09% → results/obd/resnet18_obd_s1_sp0.9.pth\n",
            "[BASELINE] acc=95.58%\n",
            "[OBD-PRUNED] sparsity=0.8992, params=11.17M\n",
            "[FT 1/5] train_acc=97.67%  test_acc=92.11%\n",
            "[FT 2/5] train_acc=98.16%  test_acc=92.43%\n",
            "[FT 3/5] train_acc=98.23%  test_acc=90.68%\n",
            "[FT 4/5] train_acc=98.50%  test_acc=92.39%\n",
            "[FT 5/5] train_acc=98.47%  test_acc=92.09%\n",
            "[DONE][OBD] sp=0.9 acc=92.09% → results/obd/resnet18_obd_s2_sp0.9.pth\n",
            "[BASELINE] acc=95.58%\n",
            "[OBD-PRUNED] sparsity=0.8992, params=11.17M\n",
            "[FT 1/5] train_acc=97.49%  test_acc=91.61%\n",
            "[FT 2/5] train_acc=98.03%  test_acc=92.23%\n",
            "[FT 3/5] train_acc=98.10%  test_acc=91.52%\n",
            "[FT 4/5] train_acc=98.34%  test_acc=92.40%\n",
            "[FT 5/5] train_acc=98.48%  test_acc=92.37%\n",
            "[DONE][OBD] sp=0.9 acc=92.37% → results/obd/resnet18_obd_s3_sp0.9.pth\n",
            "[BASELINE] acc=95.58%\n",
            "[OBD-PRUNED] sparsity=0.9492, params=11.17M\n",
            "[FT 1/5] train_acc=91.73%  test_acc=89.28%\n",
            "[FT 2/5] train_acc=96.26%  test_acc=91.76%\n",
            "[FT 3/5] train_acc=96.92%  test_acc=91.46%\n",
            "[FT 4/5] train_acc=97.27%  test_acc=92.21%\n",
            "[FT 5/5] train_acc=97.62%  test_acc=90.91%\n",
            "[DONE][OBD] sp=0.95 acc=90.91% → results/obd/resnet18_obd_s1_sp0.95.pth\n",
            "[BASELINE] acc=95.58%\n",
            "[OBD-PRUNED] sparsity=0.9492, params=11.17M\n",
            "[FT 1/5] train_acc=92.11%  test_acc=89.30%\n",
            "[FT 2/5] train_acc=96.11%  test_acc=91.43%\n",
            "[FT 3/5] train_acc=96.85%  test_acc=90.95%\n",
            "[FT 4/5] train_acc=97.24%  test_acc=90.87%\n",
            "[FT 5/5] train_acc=97.54%  test_acc=92.55%\n",
            "[DONE][OBD] sp=0.95 acc=92.55% → results/obd/resnet18_obd_s2_sp0.95.pth\n",
            "[BASELINE] acc=95.58%\n",
            "[OBD-PRUNED] sparsity=0.9492, params=11.17M\n",
            "[FT 1/5] train_acc=91.96%  test_acc=89.91%\n",
            "[FT 2/5] train_acc=95.97%  test_acc=91.87%\n",
            "[FT 3/5] train_acc=96.69%  test_acc=92.26%\n",
            "[FT 4/5] train_acc=97.17%  test_acc=91.63%\n",
            "[FT 5/5] train_acc=97.52%  test_acc=91.88%\n",
            "[DONE][OBD] sp=0.95 acc=91.88% → results/obd/resnet18_obd_s3_sp0.95.pth\n",
            "[BASELINE] acc=95.58%\n",
            "[OBD-PRUNED] sparsity=0.9792, params=11.17M\n",
            "[FT 1/5] train_acc=74.77%  test_acc=88.58%\n",
            "[FT 2/5] train_acc=92.84%  test_acc=90.14%\n",
            "[FT 3/5] train_acc=94.48%  test_acc=88.58%\n",
            "[FT 4/5] train_acc=95.44%  test_acc=91.04%\n",
            "[FT 5/5] train_acc=95.94%  test_acc=91.19%\n",
            "[DONE][OBD] sp=0.98 acc=91.19% → results/obd/resnet18_obd_s1_sp0.98.pth\n",
            "[BASELINE] acc=95.58%\n",
            "[OBD-PRUNED] sparsity=0.9792, params=11.17M\n",
            "[FT 1/5] train_acc=77.05%  test_acc=85.16%\n",
            "[FT 2/5] train_acc=92.77%  test_acc=89.35%\n",
            "[FT 3/5] train_acc=94.28%  test_acc=89.82%\n",
            "[FT 4/5] train_acc=95.27%  test_acc=91.45%\n",
            "[FT 5/5] train_acc=95.86%  test_acc=90.91%\n",
            "[DONE][OBD] sp=0.98 acc=90.91% → results/obd/resnet18_obd_s2_sp0.98.pth\n",
            "[BASELINE] acc=95.58%\n",
            "[OBD-PRUNED] sparsity=0.9792, params=11.17M\n",
            "[FT 1/5] train_acc=75.71%  test_acc=88.74%\n",
            "[FT 2/5] train_acc=92.57%  test_acc=89.73%\n",
            "[FT 3/5] train_acc=94.13%  test_acc=89.87%\n",
            "[FT 4/5] train_acc=95.08%  test_acc=90.09%\n",
            "[FT 5/5] train_acc=95.64%  test_acc=90.09%\n",
            "[DONE][OBD] sp=0.98 acc=90.09% → results/obd/resnet18_obd_s3_sp0.98.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------\n",
        "# H_diag ≈ E[g^2] 추정 (OBD와 동일)\n",
        "# --------------------------\n",
        "def estimate_hessian_diag_eg2(model, loader, device, max_batches=100):\n",
        "    \"\"\"\n",
        "    H_ii ≈ E[g_i^2]  (Conv/Linear의 weight만 대상)\n",
        "    \"\"\"\n",
        "    model.train()  # BN 통계 안정화\n",
        "    ce = nn.CrossEntropyLoss()\n",
        "\n",
        "    # 프루닝 대상 weight만 모으기\n",
        "    modules, params = [], []\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, (nn.Conv2d, nn.Linear)) and m.weight.requires_grad:\n",
        "            modules.append(m)\n",
        "            params.append(m.weight)\n",
        "\n",
        "    hdiag = [torch.zeros_like(p, device=device) for p in params]\n",
        "    steps = 0\n",
        "\n",
        "    for x,y in loader:\n",
        "        x,y = x.to(device), y.to(device)\n",
        "        model.zero_grad(set_to_none=True)\n",
        "        logits = model(x)\n",
        "        loss = ce(logits, y)\n",
        "        grads = torch.autograd.grad(loss, params, retain_graph=False, create_graph=False)\n",
        "        for hd, g in zip(hdiag, grads):\n",
        "            hd += (g.detach() ** 2)\n",
        "        steps += 1\n",
        "        if steps >= max_batches:\n",
        "            break\n",
        "\n",
        "    for i in range(len(hdiag)):\n",
        "        hdiag[i] /= max(1, steps)\n",
        "    return hdiag, modules  # modules[i].weight ↔ hdiag[i]\n",
        "\n",
        "# --------------------------\n",
        "# OBS-fast 프루닝 (글로벌)\n",
        "# --------------------------\n",
        "def obs_fast_prune_global(model, loader, device, sparsity=0.9, lambda_damp=1e-3, max_batches=100, hdiag_precomputed=None):\n",
        "    \"\"\"\n",
        "    점수: S_i ≈ 0.5 * w_i^2 * (H_ii + λ)\n",
        "    가장 작은 점수부터 'sparsity' 비율만큼 0으로.\n",
        "    hdiag_precomputed를 주면 재추정 생략 가능(OBD에서 계산한 것을 재사용).\n",
        "    \"\"\"\n",
        "    if hdiag_precomputed is None:\n",
        "        hdiag_list, modules = estimate_hessian_diag_eg2(model, loader, device, max_batches=max_batches)\n",
        "    else:\n",
        "        hdiag_list, modules = hdiag_precomputed\n",
        "\n",
        "    # 전 레이어 점수 모으기\n",
        "    scores_flat = []\n",
        "    for m, Hd in zip(modules, hdiag_list):\n",
        "        if Hd is None:   # ✅ None 값 건너뛰기\n",
        "            continue\n",
        "        W = m.weight.data\n",
        "        score = 0.5 * (W ** 2) * (Hd + lambda_damp)\n",
        "        scores_flat.append(score.flatten())\n",
        "    scores_flat = torch.cat(scores_flat)\n",
        "\n",
        "    # 전역 임계값\n",
        "    k = int(scores_flat.numel() * sparsity)\n",
        "    if k > 0:\n",
        "        thresh = torch.topk(scores_flat, k, largest=False).values.max()\n",
        "    else:\n",
        "        thresh = scores_flat.min() - 1  # 아무 것도 제거하지 않음\n",
        "\n",
        "    # 마스킹 적용(0으로 설정)\n",
        "    for m, Hd in zip(modules, hdiag_list):\n",
        "        W = m.weight.data\n",
        "        s_local = 0.5 * (W**2) * (Hd + lambda_damp)\n",
        "        W[s_local <= thresh] = 0.0\n",
        "\n",
        "    return model, (hdiag_list, modules)\n",
        "\n",
        "# --------------------------\n",
        "# 통합 실행: ckpt 로드 → OBS-fast → (짧게) 파인튜닝 → 저장\n",
        "# --------------------------\n",
        "def run_obs_once(ckpt_path, save_path, sparsity=0.9, seed=1, finetune_epochs=5, lr=1e-2,\n",
        "                 max_batches=100, lambda_damp=1e-3, reuse_hdiag=False, hdiag_cache=None):\n",
        "    \"\"\"\n",
        "    kuangliu/pytorch-cifar 포맷('net') 지원.\n",
        "    reuse_hdiag=True 이고 hdiag_cache가 있으면 E[g^2] 재계산 생략.\n",
        "    \"\"\"\n",
        "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # 데이터\n",
        "    train_loader, test_loader = get_loaders(batch_train=256, batch_test=512, workers=2)\n",
        "\n",
        "    # 모델 & ckpt 로드\n",
        "    model = resnet.ResNet18().to(device)\n",
        "    ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
        "    state = ckpt.get(\"net\", ckpt.get(\"model\", ckpt))\n",
        "    new_state = OrderedDict((k.replace(\"module.\",\"\",1) if k.startswith(\"module.\") else k, v) for k,v in state.items())\n",
        "    model.load_state_dict(new_state, strict=True)\n",
        "\n",
        "    # 프루닝 전 성능\n",
        "    base_acc, _ = evaluate(model, test_loader, device)\n",
        "    print(f\"[BASELINE] acc={base_acc:.2f}%\")\n",
        "\n",
        "    # OBS-fast 프루닝 (E[g^2] 신규/재사용)\n",
        "    hdiag_pre = hdiag_cache if reuse_hdiag else None\n",
        "    model, hdiag_pack = obs_fast_prune_global(\n",
        "        model, train_loader, device,\n",
        "        sparsity=sparsity, lambda_damp=lambda_damp, max_batches=max_batches,\n",
        "        hdiag_precomputed=hdiag_pre\n",
        "    )\n",
        "\n",
        "    total, nnz = count_params(model)\n",
        "    print(f\"[OBS-PRUNED] sparsity={1 - nnz/total:.4f}, params={total/1e6:.2f}M, lambda={lambda_damp}\")\n",
        "\n",
        "    # 짧은 파인튜닝\n",
        "    if finetune_epochs > 0:\n",
        "        ce = nn.CrossEntropyLoss()\n",
        "        opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4, nesterov=True)\n",
        "        for ep in range(finetune_epochs):\n",
        "            model.train()\n",
        "            loss_sum = cor = tot = 0\n",
        "            for x,y in train_loader:\n",
        "                x,y = x.to(device), y.to(device)\n",
        "                opt.zero_grad(set_to_none=True)\n",
        "                logits = model(x); loss = ce(logits,y)\n",
        "                loss.backward(); opt.step()\n",
        "                loss_sum += loss.item()*x.size(0)\n",
        "                cor += (logits.argmax(1)==y).sum().item()\n",
        "                tot += y.numel()\n",
        "            tr_acc = 100*cor/tot\n",
        "            te_acc, _ = evaluate(model, test_loader, device)\n",
        "            print(f\"[FT {ep+1}/{finetune_epochs}] train_acc={tr_acc:.2f}%  test_acc={te_acc:.2f}%\")\n",
        "\n",
        "    # 저장 (kuangliu 포맷)\n",
        "    acc, _ = evaluate(model, test_loader, device)\n",
        "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "    torch.save({\n",
        "        \"net\": model.state_dict(),\n",
        "        \"acc\": float(acc),\n",
        "        \"base_acc\": float(base_acc),\n",
        "        \"sparsity\": float(sparsity),\n",
        "        \"seed\": int(seed),\n",
        "        \"params_total\": int(total),\n",
        "        \"params_nnz\": int(nnz),\n",
        "        \"method\": \"obs\",\n",
        "        \"lambda_damp\": float(lambda_damp),\n",
        "        \"hdiag_batches\": int(max_batches),\n",
        "    }, save_path)\n",
        "    print(f\"[DONE][OBS] sp={sparsity} acc={acc:.2f}% → {save_path}\")\n",
        "    return hdiag_pack  # (hdiag_list, modules)"
      ],
      "metadata": {
        "id": "QZ4TeBtwEgFY"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ckpt_path = \"checkpoint/ResNet18.pth\"   # 베이스라인 ckpt\n",
        "os.makedirs(\"results/obs\", exist_ok=True)\n",
        "\n",
        "sparsities = [0.00, 0.20, 0.40, 0.60, 0.80, 0.90, 0.95, 0.98]\n",
        "seeds = [1, 2, 3]\n",
        "\n",
        "# λ는 1e-3 ~ 1e-2 구간이 무난. 시간 없으면 1e-3로 통일.\n",
        "lambda_damp = 1e-3\n",
        "max_batches = 100\n",
        "\n",
        "for sp in sparsities:\n",
        "    for s in seeds:\n",
        "        run_obs_once(\n",
        "            ckpt_path=ckpt_path,\n",
        "            save_path=f\"results/obs/resnet18_obs_s{s}_sp{sp}.pth\",\n",
        "            sparsity=sp, seed=s,\n",
        "            finetune_epochs=5, lr=1e-2,\n",
        "            max_batches=max_batches, lambda_damp=lambda_damp\n",
        "        )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZ0B5juKS3Ik",
        "outputId": "81116dfd-698d-44de-8a4a-a672442be371"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[BASELINE] acc=95.58%\n",
            "[OBS-PRUNED] sparsity=0.0000, params=11.17M, lambda=0.001\n",
            "[FT 1/5] train_acc=99.99%  test_acc=95.61%\n",
            "[FT 2/5] train_acc=99.99%  test_acc=95.47%\n",
            "[FT 3/5] train_acc=100.00%  test_acc=95.52%\n",
            "[FT 4/5] train_acc=99.99%  test_acc=95.48%\n",
            "[FT 5/5] train_acc=100.00%  test_acc=95.36%\n",
            "[DONE][OBS] sp=0.0 acc=95.36% → results/obs/resnet18_obs_s1_sp0.0.pth\n",
            "[BASELINE] acc=95.58%\n",
            "[OBS-PRUNED] sparsity=0.0000, params=11.17M, lambda=0.001\n",
            "[FT 1/5] train_acc=100.00%  test_acc=95.55%\n",
            "[FT 2/5] train_acc=100.00%  test_acc=95.45%\n",
            "[FT 3/5] train_acc=99.99%  test_acc=95.36%\n",
            "[FT 4/5] train_acc=100.00%  test_acc=95.58%\n",
            "[FT 5/5] train_acc=100.00%  test_acc=95.62%\n",
            "[DONE][OBS] sp=0.0 acc=95.62% → results/obs/resnet18_obs_s2_sp0.0.pth\n",
            "[BASELINE] acc=95.58%\n",
            "[OBS-PRUNED] sparsity=0.0000, params=11.17M, lambda=0.001\n",
            "[FT 1/5] train_acc=100.00%  test_acc=95.49%\n",
            "[FT 2/5] train_acc=100.00%  test_acc=95.49%\n",
            "[FT 3/5] train_acc=100.00%  test_acc=95.47%\n",
            "[FT 4/5] train_acc=100.00%  test_acc=95.46%\n",
            "[FT 5/5] train_acc=100.00%  test_acc=95.46%\n",
            "[DONE][OBS] sp=0.0 acc=95.46% → results/obs/resnet18_obs_s3_sp0.0.pth\n",
            "[BASELINE] acc=95.58%\n",
            "[OBS-PRUNED] sparsity=0.1998, params=11.17M, lambda=0.001\n",
            "[FT 1/5] train_acc=100.00%  test_acc=95.62%\n",
            "[FT 2/5] train_acc=99.99%  test_acc=95.46%\n",
            "[FT 3/5] train_acc=100.00%  test_acc=95.51%\n",
            "[FT 4/5] train_acc=99.99%  test_acc=95.50%\n",
            "[FT 5/5] train_acc=100.00%  test_acc=95.34%\n",
            "[DONE][OBS] sp=0.2 acc=95.34% → results/obs/resnet18_obs_s1_sp0.2.pth\n",
            "[BASELINE] acc=95.58%\n",
            "[OBS-PRUNED] sparsity=0.1998, params=11.17M, lambda=0.001\n",
            "[FT 1/5] train_acc=100.00%  test_acc=95.53%\n",
            "[FT 2/5] train_acc=100.00%  test_acc=95.46%\n",
            "[FT 3/5] train_acc=99.99%  test_acc=95.36%\n",
            "[FT 4/5] train_acc=100.00%  test_acc=95.58%\n",
            "[FT 5/5] train_acc=100.00%  test_acc=95.62%\n",
            "[DONE][OBS] sp=0.2 acc=95.62% → results/obs/resnet18_obs_s2_sp0.2.pth\n",
            "[BASELINE] acc=95.58%\n",
            "[OBS-PRUNED] sparsity=0.1998, params=11.17M, lambda=0.001\n",
            "[FT 1/5] train_acc=100.00%  test_acc=95.50%\n",
            "[FT 2/5] train_acc=100.00%  test_acc=95.50%\n",
            "[FT 3/5] train_acc=100.00%  test_acc=95.50%\n",
            "[FT 4/5] train_acc=100.00%  test_acc=95.46%\n",
            "[FT 5/5] train_acc=100.00%  test_acc=95.48%\n",
            "[DONE][OBS] sp=0.2 acc=95.48% → results/obs/resnet18_obs_s3_sp0.2.pth\n",
            "[BASELINE] acc=95.58%\n",
            "[OBS-PRUNED] sparsity=0.3997, params=11.17M, lambda=0.001\n",
            "[FT 1/5] train_acc=100.00%  test_acc=95.60%\n",
            "[FT 2/5] train_acc=99.99%  test_acc=95.44%\n",
            "[FT 3/5] train_acc=100.00%  test_acc=95.54%\n",
            "[FT 4/5] train_acc=99.99%  test_acc=95.44%\n",
            "[FT 5/5] train_acc=100.00%  test_acc=95.37%\n",
            "[DONE][OBS] sp=0.4 acc=95.37% → results/obs/resnet18_obs_s1_sp0.4.pth\n",
            "[BASELINE] acc=95.58%\n",
            "[OBS-PRUNED] sparsity=0.3997, params=11.17M, lambda=0.001\n",
            "[FT 1/5] train_acc=100.00%  test_acc=95.52%\n",
            "[FT 2/5] train_acc=100.00%  test_acc=95.49%\n",
            "[FT 3/5] train_acc=99.99%  test_acc=95.39%\n",
            "[FT 4/5] train_acc=100.00%  test_acc=95.57%\n",
            "[FT 5/5] train_acc=100.00%  test_acc=95.65%\n",
            "[DONE][OBS] sp=0.4 acc=95.65% → results/obs/resnet18_obs_s2_sp0.4.pth\n",
            "[BASELINE] acc=95.58%\n",
            "[OBS-PRUNED] sparsity=0.3997, params=11.17M, lambda=0.001\n",
            "[FT 1/5] train_acc=100.00%  test_acc=95.55%\n",
            "[FT 2/5] train_acc=100.00%  test_acc=95.50%\n",
            "[FT 3/5] train_acc=100.00%  test_acc=95.51%\n",
            "[FT 4/5] train_acc=100.00%  test_acc=95.46%\n",
            "[FT 5/5] train_acc=100.00%  test_acc=95.48%\n",
            "[DONE][OBS] sp=0.4 acc=95.48% → results/obs/resnet18_obs_s3_sp0.4.pth\n",
            "[BASELINE] acc=95.58%\n",
            "[OBS-PRUNED] sparsity=0.5995, params=11.17M, lambda=0.001\n",
            "[FT 1/5] train_acc=99.99%  test_acc=95.47%\n",
            "[FT 2/5] train_acc=100.00%  test_acc=95.49%\n",
            "[FT 3/5] train_acc=100.00%  test_acc=95.53%\n",
            "[FT 4/5] train_acc=99.99%  test_acc=95.44%\n",
            "[FT 5/5] train_acc=100.00%  test_acc=95.37%\n",
            "[DONE][OBS] sp=0.6 acc=95.37% → results/obs/resnet18_obs_s1_sp0.6.pth\n",
            "[BASELINE] acc=95.58%\n",
            "[OBS-PRUNED] sparsity=0.5995, params=11.17M, lambda=0.001\n",
            "[FT 1/5] train_acc=100.00%  test_acc=95.42%\n",
            "[FT 2/5] train_acc=100.00%  test_acc=95.52%\n",
            "[FT 3/5] train_acc=100.00%  test_acc=95.45%\n",
            "[FT 4/5] train_acc=100.00%  test_acc=95.62%\n",
            "[FT 5/5] train_acc=100.00%  test_acc=95.64%\n",
            "[DONE][OBS] sp=0.6 acc=95.64% → results/obs/resnet18_obs_s2_sp0.6.pth\n",
            "[BASELINE] acc=95.58%\n",
            "[OBS-PRUNED] sparsity=0.5995, params=11.17M, lambda=0.001\n",
            "[FT 1/5] train_acc=100.00%  test_acc=95.47%\n",
            "[FT 2/5] train_acc=100.00%  test_acc=95.43%\n",
            "[FT 3/5] train_acc=100.00%  test_acc=95.54%\n",
            "[FT 4/5] train_acc=99.99%  test_acc=95.50%\n",
            "[FT 5/5] train_acc=100.00%  test_acc=95.55%\n",
            "[DONE][OBS] sp=0.6 acc=95.55% → results/obs/resnet18_obs_s3_sp0.6.pth\n",
            "[BASELINE] acc=95.58%\n",
            "[OBS-PRUNED] sparsity=0.7993, params=11.17M, lambda=0.001\n",
            "[FT 1/5] train_acc=99.97%  test_acc=95.19%\n",
            "[FT 2/5] train_acc=99.98%  test_acc=95.43%\n",
            "[FT 3/5] train_acc=100.00%  test_acc=95.41%\n",
            "[FT 4/5] train_acc=99.97%  test_acc=94.98%\n",
            "[FT 5/5] train_acc=99.96%  test_acc=94.94%\n",
            "[DONE][OBS] sp=0.8 acc=94.94% → results/obs/resnet18_obs_s1_sp0.8.pth\n",
            "[BASELINE] acc=95.58%\n",
            "[OBS-PRUNED] sparsity=0.7993, params=11.17M, lambda=0.001\n",
            "[FT 1/5] train_acc=99.98%  test_acc=95.34%\n",
            "[FT 2/5] train_acc=99.98%  test_acc=95.56%\n",
            "[FT 3/5] train_acc=99.98%  test_acc=95.15%\n",
            "[FT 4/5] train_acc=99.96%  test_acc=94.96%\n",
            "[FT 5/5] train_acc=99.95%  test_acc=95.20%\n",
            "[DONE][OBS] sp=0.8 acc=95.20% → results/obs/resnet18_obs_s2_sp0.8.pth\n",
            "[BASELINE] acc=95.58%\n",
            "[OBS-PRUNED] sparsity=0.7993, params=11.17M, lambda=0.001\n",
            "[FT 1/5] train_acc=99.95%  test_acc=95.22%\n",
            "[FT 2/5] train_acc=99.99%  test_acc=95.40%\n",
            "[FT 3/5] train_acc=99.99%  test_acc=95.25%\n",
            "[FT 4/5] train_acc=99.98%  test_acc=95.29%\n",
            "[FT 5/5] train_acc=99.99%  test_acc=95.06%\n",
            "[DONE][OBS] sp=0.8 acc=95.06% → results/obs/resnet18_obs_s3_sp0.8.pth\n",
            "[BASELINE] acc=95.58%\n",
            "[OBS-PRUNED] sparsity=0.8992, params=11.17M, lambda=0.001\n",
            "[FT 1/5] train_acc=99.31%  test_acc=93.07%\n",
            "[FT 2/5] train_acc=98.93%  test_acc=92.57%\n",
            "[FT 3/5] train_acc=98.79%  test_acc=92.64%\n",
            "[FT 4/5] train_acc=98.86%  test_acc=92.77%\n",
            "[FT 5/5] train_acc=98.88%  test_acc=92.83%\n",
            "[DONE][OBS] sp=0.9 acc=92.83% → results/obs/resnet18_obs_s1_sp0.9.pth\n",
            "[BASELINE] acc=95.58%\n",
            "[OBS-PRUNED] sparsity=0.8992, params=11.17M, lambda=0.001\n",
            "[FT 1/5] train_acc=99.23%  test_acc=93.03%\n",
            "[FT 2/5] train_acc=98.87%  test_acc=92.28%\n",
            "[FT 3/5] train_acc=98.78%  test_acc=92.93%\n",
            "[FT 4/5] train_acc=98.91%  test_acc=93.07%\n",
            "[FT 5/5] train_acc=98.94%  test_acc=92.88%\n",
            "[DONE][OBS] sp=0.9 acc=92.88% → results/obs/resnet18_obs_s2_sp0.9.pth\n",
            "[BASELINE] acc=95.58%\n",
            "[OBS-PRUNED] sparsity=0.8992, params=11.17M, lambda=0.001\n",
            "[FT 1/5] train_acc=99.29%  test_acc=92.96%\n",
            "[FT 2/5] train_acc=99.02%  test_acc=92.89%\n",
            "[FT 3/5] train_acc=98.79%  test_acc=93.16%\n",
            "[FT 4/5] train_acc=98.89%  test_acc=92.50%\n",
            "[FT 5/5] train_acc=98.77%  test_acc=92.37%\n",
            "[DONE][OBS] sp=0.9 acc=92.37% → results/obs/resnet18_obs_s3_sp0.9.pth\n",
            "[BASELINE] acc=95.58%\n",
            "[OBS-PRUNED] sparsity=0.9492, params=11.17M, lambda=0.001\n",
            "[FT 1/5] train_acc=95.82%  test_acc=91.34%\n",
            "[FT 2/5] train_acc=97.07%  test_acc=91.63%\n",
            "[FT 3/5] train_acc=97.41%  test_acc=91.96%\n",
            "[FT 4/5] train_acc=97.75%  test_acc=91.93%\n",
            "[FT 5/5] train_acc=97.81%  test_acc=91.49%\n",
            "[DONE][OBS] sp=0.95 acc=91.49% → results/obs/resnet18_obs_s1_sp0.95.pth\n",
            "[BASELINE] acc=95.58%\n",
            "[OBS-PRUNED] sparsity=0.9492, params=11.17M, lambda=0.001\n",
            "[FT 1/5] train_acc=95.83%  test_acc=91.28%\n",
            "[FT 2/5] train_acc=96.99%  test_acc=91.38%\n",
            "[FT 3/5] train_acc=97.32%  test_acc=91.34%\n",
            "[FT 4/5] train_acc=97.85%  test_acc=92.23%\n",
            "[FT 5/5] train_acc=98.07%  test_acc=92.29%\n",
            "[DONE][OBS] sp=0.95 acc=92.29% → results/obs/resnet18_obs_s2_sp0.95.pth\n",
            "[BASELINE] acc=95.58%\n",
            "[OBS-PRUNED] sparsity=0.9492, params=11.17M, lambda=0.001\n",
            "[FT 1/5] train_acc=95.74%  test_acc=88.14%\n",
            "[FT 2/5] train_acc=96.88%  test_acc=90.49%\n",
            "[FT 3/5] train_acc=97.61%  test_acc=91.38%\n",
            "[FT 4/5] train_acc=97.85%  test_acc=91.38%\n",
            "[FT 5/5] train_acc=98.05%  test_acc=92.13%\n",
            "[DONE][OBS] sp=0.95 acc=92.13% → results/obs/resnet18_obs_s3_sp0.95.pth\n",
            "[BASELINE] acc=95.58%\n",
            "[OBS-PRUNED] sparsity=0.9792, params=11.17M, lambda=0.001\n",
            "[FT 1/5] train_acc=89.15%  test_acc=89.73%\n",
            "[FT 2/5] train_acc=94.24%  test_acc=90.17%\n",
            "[FT 3/5] train_acc=95.42%  test_acc=91.12%\n",
            "[FT 4/5] train_acc=96.17%  test_acc=90.23%\n",
            "[FT 5/5] train_acc=96.60%  test_acc=91.56%\n",
            "[DONE][OBS] sp=0.98 acc=91.56% → results/obs/resnet18_obs_s1_sp0.98.pth\n",
            "[BASELINE] acc=95.58%\n",
            "[OBS-PRUNED] sparsity=0.9792, params=11.17M, lambda=0.001\n",
            "[FT 1/5] train_acc=88.70%  test_acc=89.80%\n",
            "[FT 2/5] train_acc=94.14%  test_acc=90.36%\n",
            "[FT 3/5] train_acc=95.31%  test_acc=88.43%\n",
            "[FT 4/5] train_acc=96.03%  test_acc=91.66%\n",
            "[FT 5/5] train_acc=96.71%  test_acc=91.83%\n",
            "[DONE][OBS] sp=0.98 acc=91.83% → results/obs/resnet18_obs_s2_sp0.98.pth\n",
            "[BASELINE] acc=95.58%\n",
            "[OBS-PRUNED] sparsity=0.9792, params=11.17M, lambda=0.001\n",
            "[FT 1/5] train_acc=88.86%  test_acc=85.94%\n",
            "[FT 2/5] train_acc=94.18%  test_acc=90.62%\n",
            "[FT 3/5] train_acc=95.43%  test_acc=91.11%\n",
            "[FT 4/5] train_acc=96.06%  test_acc=90.64%\n",
            "[FT 5/5] train_acc=96.68%  test_acc=90.84%\n",
            "[DONE][OBS] sp=0.98 acc=90.84% → results/obs/resnet18_obs_s3_sp0.98.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r36ZNjB0S7tk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}